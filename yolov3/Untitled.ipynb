{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolov3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6eb6814f9466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yolov3/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myolov3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolov3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolov3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolov3'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [3, 3]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#!conda install -c conda-forge opencv -y\n",
    "import cv2 \n",
    "from utils import *\n",
    "from utils.utils import *\n",
    "from models import Darknet\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "!pip install shapely\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "random.shuffle(labeled_scene_index)\n",
    "labeled_scene_index_train = labeled_scene_index[0:21]\n",
    "labeled_scene_index_val = labeled_scene_index[21:28]\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "print(\"Train scenes: {} \\nVal scenes: {}\".format(len(labeled_scene_index_train), len(labeled_scene_index_val)))\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_val,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = \"cfg/yolov3-tiny-3cls.cfg\"\n",
    "config = \"cfg/yolov3-spp.cfg\"\n",
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 0,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'lr0': 0.0001,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 0.00005,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.000484,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Darknet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8516d3ab3900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_file2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_bounding_box.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Darknet' is not defined"
     ]
    }
   ],
   "source": [
    "model_file2 = 'best_bounding_box.pt'\n",
    "model = Darknet(config, verbose=False).to(device)\n",
    "checkpoint2 = torch.load(model_file2)\n",
    "model.load_state_dict(checkpoint2['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'config', 'model_state_dict', 'conv_state_dict', 'optimizer_state_dict', 'val_loss'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = 3*256*306\n",
    "d = 20\n",
    "s = 300\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size= (7,3), stride= 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size= 3, stride= 3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size= 3, stride= 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size= 5, stride= 3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size= 3, stride= 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2,True)\n",
    "        )\n",
    "        self.l1=nn.Sequential(\n",
    "            nn.Linear(1024*48, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, s * 2)\n",
    "        )\n",
    "        self.l2=nn.Sequential(\n",
    "            nn.Linear(s, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, 1024*48)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=(7,3), stride=3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        mu_logvar = self.l1(enc.view(len(x),-1)).view(-1, 2, s)\n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        dec = self.decoder(self.l2(z).view(len(z),1024, 6, 8))\n",
    "        return dec, mu, logvar\n",
    "\n",
    "model_vae = VAE().to(device)\n",
    "checkpoint = torch.load('/home/yt1526/VAE')\n",
    "model_vae.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(18, 3, kernel_size = (3, 3), padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = conv().to(device)\n",
    "conv.load_state_dict(checkpoint2['conv_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "for k, v in dict(model.named_parameters()).items():\n",
    "    if '.bias' in k:\n",
    "        if v.is_leaf:\n",
    "            pg2 += [v]  # biases\n",
    "    elif 'Conv2d.weight' in k:\n",
    "        pg1 += [v]  # apply weight_decay\n",
    "    else:\n",
    "        pg0 += [v]  # all else\n",
    "\n",
    "optimizer = optim.Adam(list(pg0)+list(conv.parameters()), lr=hyp['lr0'])\n",
    "optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "del pg0, pg1, pg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "def train(train_loader, model, optimizer, criterion, epoch):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(train_loader):\n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        batch_size = sample.shape[0]\n",
    "        sample = sample.view(batch_size, 18, 256, 306)\n",
    "        sample = conv(sample)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "        # Run through model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample)\n",
    "\n",
    "        # Calculate loss and take step\n",
    "        loss, loss_items = compute_loss(output, target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_loader, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(val_loader):\n",
    "        \n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        # \"Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\"\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "        \n",
    "        sample = torch.stack(sample).to(device)\n",
    "        batch_size = sample.shape[0]\n",
    "        sample = sample.view(batch_size, 18, 256, 306)\n",
    "        sample = conv(sample)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "        # Run through model\n",
    "        with torch.no_grad():\n",
    "            output = model(sample)\n",
    "        loss, loss_items = compute_loss(output[1], target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss = sum(losses)/len(losses)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 [0/2646 (0%)]\tLoss: 7.886873\n",
      "\tTrain Epoch: 0 [200/2646 (8%)]\tLoss: 3.208977\n",
      "\tTrain Epoch: 0 [400/2646 (15%)]\tLoss: 3.780180\n",
      "\tTrain Epoch: 0 [600/2646 (23%)]\tLoss: 4.643973\n",
      "\tTrain Epoch: 0 [800/2646 (30%)]\tLoss: 8.206614\n",
      "\tTrain Epoch: 0 [1000/2646 (38%)]\tLoss: 4.041626\n",
      "\tTrain Epoch: 0 [1200/2646 (45%)]\tLoss: 5.106427\n",
      "\tTrain Epoch: 0 [1400/2646 (53%)]\tLoss: 2.939772\n",
      "\tTrain Epoch: 0 [1600/2646 (60%)]\tLoss: 3.744762\n",
      "\tTrain Epoch: 0 [1800/2646 (68%)]\tLoss: 3.499445\n",
      "\tTrain Epoch: 0 [2000/2646 (76%)]\tLoss: 4.986573\n",
      "\tTrain Epoch: 0 [2200/2646 (83%)]\tLoss: 4.469500\n",
      "\tTrain Epoch: 0 [2400/2646 (91%)]\tLoss: 3.302412\n",
      "\tTrain Epoch: 0 [2600/2646 (98%)]\tLoss: 2.766295\n",
      "Evaluating after Epoch 0:\n",
      "Val loss is tensor([4.91250])\n",
      "\tTrain Epoch: 1 [0/2646 (0%)]\tLoss: 3.266164\n",
      "\tTrain Epoch: 1 [200/2646 (8%)]\tLoss: 4.160567\n",
      "\tTrain Epoch: 1 [400/2646 (15%)]\tLoss: 4.717836\n",
      "\tTrain Epoch: 1 [600/2646 (23%)]\tLoss: 3.686123\n",
      "\tTrain Epoch: 1 [800/2646 (30%)]\tLoss: 2.753664\n",
      "\tTrain Epoch: 1 [1000/2646 (38%)]\tLoss: 3.520462\n",
      "\tTrain Epoch: 1 [1200/2646 (45%)]\tLoss: 4.993030\n",
      "\tTrain Epoch: 1 [1400/2646 (53%)]\tLoss: 5.740582\n",
      "\tTrain Epoch: 1 [1600/2646 (60%)]\tLoss: 2.698454\n",
      "\tTrain Epoch: 1 [1800/2646 (68%)]\tLoss: 4.048398\n",
      "\tTrain Epoch: 1 [2000/2646 (76%)]\tLoss: 4.361062\n",
      "\tTrain Epoch: 1 [2200/2646 (83%)]\tLoss: 3.501460\n",
      "\tTrain Epoch: 1 [2400/2646 (91%)]\tLoss: 3.970843\n",
      "\tTrain Epoch: 1 [2600/2646 (98%)]\tLoss: 2.525005\n",
      "Evaluating after Epoch 1:\n",
      "Val loss is tensor([4.75462])\n",
      "\tTrain Epoch: 2 [0/2646 (0%)]\tLoss: 3.707570\n",
      "\tTrain Epoch: 2 [200/2646 (8%)]\tLoss: 3.316061\n",
      "\tTrain Epoch: 2 [400/2646 (15%)]\tLoss: 3.139901\n",
      "\tTrain Epoch: 2 [600/2646 (23%)]\tLoss: 3.963446\n",
      "\tTrain Epoch: 2 [800/2646 (30%)]\tLoss: 4.675414\n",
      "\tTrain Epoch: 2 [1000/2646 (38%)]\tLoss: 4.334560\n",
      "\tTrain Epoch: 2 [1200/2646 (45%)]\tLoss: 3.877740\n",
      "\tTrain Epoch: 2 [1400/2646 (53%)]\tLoss: 6.360011\n",
      "\tTrain Epoch: 2 [1600/2646 (60%)]\tLoss: 7.764433\n",
      "\tTrain Epoch: 2 [1800/2646 (68%)]\tLoss: 6.340409\n",
      "\tTrain Epoch: 2 [2000/2646 (76%)]\tLoss: 2.495887\n",
      "\tTrain Epoch: 2 [2200/2646 (83%)]\tLoss: 2.859151\n",
      "\tTrain Epoch: 2 [2400/2646 (91%)]\tLoss: 6.537605\n",
      "\tTrain Epoch: 2 [2600/2646 (98%)]\tLoss: 6.231532\n",
      "Evaluating after Epoch 2:\n",
      "Val loss is tensor([4.87115])\n",
      "\tTrain Epoch: 3 [0/2646 (0%)]\tLoss: 2.700725\n",
      "\tTrain Epoch: 3 [200/2646 (8%)]\tLoss: 13.577253\n",
      "\tTrain Epoch: 3 [400/2646 (15%)]\tLoss: 5.822171\n",
      "\tTrain Epoch: 3 [600/2646 (23%)]\tLoss: 3.134690\n",
      "\tTrain Epoch: 3 [800/2646 (30%)]\tLoss: 3.496873\n",
      "\tTrain Epoch: 3 [1000/2646 (38%)]\tLoss: 2.742912\n",
      "\tTrain Epoch: 3 [1200/2646 (45%)]\tLoss: 6.387477\n",
      "\tTrain Epoch: 3 [1400/2646 (53%)]\tLoss: 5.898584\n",
      "\tTrain Epoch: 3 [1600/2646 (60%)]\tLoss: 3.040412\n",
      "\tTrain Epoch: 3 [1800/2646 (68%)]\tLoss: 6.577812\n",
      "\tTrain Epoch: 3 [2000/2646 (76%)]\tLoss: 3.249780\n",
      "\tTrain Epoch: 3 [2200/2646 (83%)]\tLoss: 3.459821\n",
      "\tTrain Epoch: 3 [2400/2646 (91%)]\tLoss: 2.700203\n",
      "\tTrain Epoch: 3 [2600/2646 (98%)]\tLoss: 4.404167\n",
      "Evaluating after Epoch 3:\n",
      "Val loss is tensor([4.84656])\n",
      "\tTrain Epoch: 4 [0/2646 (0%)]\tLoss: 4.323625\n",
      "\tTrain Epoch: 4 [200/2646 (8%)]\tLoss: 2.652020\n",
      "\tTrain Epoch: 4 [400/2646 (15%)]\tLoss: 3.741644\n",
      "\tTrain Epoch: 4 [600/2646 (23%)]\tLoss: 4.433900\n",
      "\tTrain Epoch: 4 [800/2646 (30%)]\tLoss: 2.599243\n",
      "\tTrain Epoch: 4 [1000/2646 (38%)]\tLoss: 4.780602\n",
      "\tTrain Epoch: 4 [1200/2646 (45%)]\tLoss: 5.026602\n",
      "\tTrain Epoch: 4 [1400/2646 (53%)]\tLoss: 6.427435\n",
      "\tTrain Epoch: 4 [1600/2646 (60%)]\tLoss: 3.866482\n",
      "\tTrain Epoch: 4 [1800/2646 (68%)]\tLoss: 5.820049\n",
      "\tTrain Epoch: 4 [2000/2646 (76%)]\tLoss: 3.205064\n",
      "\tTrain Epoch: 4 [2200/2646 (83%)]\tLoss: 3.724246\n",
      "\tTrain Epoch: 4 [2400/2646 (91%)]\tLoss: 7.806617\n",
      "\tTrain Epoch: 4 [2600/2646 (98%)]\tLoss: 2.360962\n",
      "Evaluating after Epoch 4:\n",
      "Val loss is tensor([4.89155])\n",
      "\tTrain Epoch: 5 [0/2646 (0%)]\tLoss: 5.742694\n",
      "\tTrain Epoch: 5 [200/2646 (8%)]\tLoss: 3.963496\n",
      "\tTrain Epoch: 5 [400/2646 (15%)]\tLoss: 2.517975\n",
      "\tTrain Epoch: 5 [600/2646 (23%)]\tLoss: 5.189904\n",
      "\tTrain Epoch: 5 [800/2646 (30%)]\tLoss: 5.358522\n",
      "\tTrain Epoch: 5 [1000/2646 (38%)]\tLoss: 6.024028\n",
      "\tTrain Epoch: 5 [1200/2646 (45%)]\tLoss: 4.197286\n",
      "\tTrain Epoch: 5 [1400/2646 (53%)]\tLoss: 1.188004\n",
      "\tTrain Epoch: 5 [1600/2646 (60%)]\tLoss: 3.983644\n",
      "\tTrain Epoch: 5 [1800/2646 (68%)]\tLoss: 2.757356\n",
      "\tTrain Epoch: 5 [2000/2646 (76%)]\tLoss: 5.522571\n",
      "\tTrain Epoch: 5 [2200/2646 (83%)]\tLoss: 4.587243\n",
      "\tTrain Epoch: 5 [2400/2646 (91%)]\tLoss: 6.987006\n",
      "\tTrain Epoch: 5 [2600/2646 (98%)]\tLoss: 5.767701\n",
      "Evaluating after Epoch 5:\n",
      "Val loss is tensor([4.90949])\n",
      "\tTrain Epoch: 6 [0/2646 (0%)]\tLoss: 3.132181\n",
      "\tTrain Epoch: 6 [200/2646 (8%)]\tLoss: 3.989465\n",
      "\tTrain Epoch: 6 [400/2646 (15%)]\tLoss: 5.984110\n",
      "\tTrain Epoch: 6 [600/2646 (23%)]\tLoss: 3.022614\n",
      "\tTrain Epoch: 6 [800/2646 (30%)]\tLoss: 3.456818\n",
      "\tTrain Epoch: 6 [1000/2646 (38%)]\tLoss: 7.090430\n",
      "\tTrain Epoch: 6 [1200/2646 (45%)]\tLoss: 6.969521\n",
      "\tTrain Epoch: 6 [1400/2646 (53%)]\tLoss: 4.237477\n",
      "\tTrain Epoch: 6 [1600/2646 (60%)]\tLoss: 4.348971\n",
      "\tTrain Epoch: 6 [1800/2646 (68%)]\tLoss: 4.572878\n",
      "\tTrain Epoch: 6 [2000/2646 (76%)]\tLoss: 2.934295\n",
      "\tTrain Epoch: 6 [2200/2646 (83%)]\tLoss: 1.917779\n",
      "\tTrain Epoch: 6 [2400/2646 (91%)]\tLoss: 1.852891\n",
      "\tTrain Epoch: 6 [2600/2646 (98%)]\tLoss: 3.272238\n",
      "Evaluating after Epoch 6:\n",
      "Val loss is tensor([4.94855])\n",
      "\tTrain Epoch: 7 [0/2646 (0%)]\tLoss: 3.102158\n",
      "\tTrain Epoch: 7 [200/2646 (8%)]\tLoss: 1.552694\n",
      "\tTrain Epoch: 7 [400/2646 (15%)]\tLoss: 6.935767\n",
      "\tTrain Epoch: 7 [600/2646 (23%)]\tLoss: 3.826234\n",
      "\tTrain Epoch: 7 [800/2646 (30%)]\tLoss: 6.860922\n",
      "\tTrain Epoch: 7 [1000/2646 (38%)]\tLoss: 1.445511\n",
      "\tTrain Epoch: 7 [1200/2646 (45%)]\tLoss: 3.952929\n",
      "\tTrain Epoch: 7 [1400/2646 (53%)]\tLoss: 3.439427\n",
      "\tTrain Epoch: 7 [1600/2646 (60%)]\tLoss: 3.353631\n",
      "\tTrain Epoch: 7 [1800/2646 (68%)]\tLoss: 1.988442\n",
      "\tTrain Epoch: 7 [2000/2646 (76%)]\tLoss: 2.512146\n",
      "\tTrain Epoch: 7 [2200/2646 (83%)]\tLoss: 2.102810\n",
      "\tTrain Epoch: 7 [2400/2646 (91%)]\tLoss: 2.960668\n",
      "\tTrain Epoch: 7 [2600/2646 (98%)]\tLoss: 3.885591\n",
      "Evaluating after Epoch 7:\n",
      "Val loss is tensor([4.99176])\n",
      "\tTrain Epoch: 8 [0/2646 (0%)]\tLoss: 1.927287\n",
      "\tTrain Epoch: 8 [200/2646 (8%)]\tLoss: 3.597500\n",
      "\tTrain Epoch: 8 [400/2646 (15%)]\tLoss: 1.529236\n",
      "\tTrain Epoch: 8 [600/2646 (23%)]\tLoss: 2.859954\n",
      "\tTrain Epoch: 8 [800/2646 (30%)]\tLoss: 3.436041\n",
      "\tTrain Epoch: 8 [1000/2646 (38%)]\tLoss: 2.345352\n",
      "\tTrain Epoch: 8 [1200/2646 (45%)]\tLoss: 7.248811\n",
      "\tTrain Epoch: 8 [1400/2646 (53%)]\tLoss: 4.017226\n",
      "\tTrain Epoch: 8 [1600/2646 (60%)]\tLoss: 2.752610\n",
      "\tTrain Epoch: 8 [1800/2646 (68%)]\tLoss: 4.107137\n",
      "\tTrain Epoch: 8 [2000/2646 (76%)]\tLoss: 2.490829\n",
      "\tTrain Epoch: 8 [2200/2646 (83%)]\tLoss: 3.912497\n",
      "\tTrain Epoch: 8 [2400/2646 (91%)]\tLoss: 2.763346\n",
      "\tTrain Epoch: 8 [2600/2646 (98%)]\tLoss: 7.582196\n",
      "Evaluating after Epoch 8:\n",
      "Val loss is tensor([5.08492])\n",
      "\tTrain Epoch: 9 [0/2646 (0%)]\tLoss: 2.228653\n",
      "\tTrain Epoch: 9 [200/2646 (8%)]\tLoss: 1.987640\n",
      "\tTrain Epoch: 9 [400/2646 (15%)]\tLoss: 5.058876\n",
      "\tTrain Epoch: 9 [600/2646 (23%)]\tLoss: 1.793282\n",
      "\tTrain Epoch: 9 [800/2646 (30%)]\tLoss: 3.158020\n",
      "\tTrain Epoch: 9 [1000/2646 (38%)]\tLoss: 4.055852\n",
      "\tTrain Epoch: 9 [1200/2646 (45%)]\tLoss: 3.991475\n",
      "\tTrain Epoch: 9 [1400/2646 (53%)]\tLoss: 3.915082\n",
      "\tTrain Epoch: 9 [1600/2646 (60%)]\tLoss: 5.173422\n",
      "\tTrain Epoch: 9 [1800/2646 (68%)]\tLoss: 2.162484\n",
      "\tTrain Epoch: 9 [2000/2646 (76%)]\tLoss: 6.486364\n",
      "\tTrain Epoch: 9 [2200/2646 (83%)]\tLoss: 4.835611\n",
      "\tTrain Epoch: 9 [2400/2646 (91%)]\tLoss: 1.939842\n",
      "\tTrain Epoch: 9 [2600/2646 (98%)]\tLoss: 2.071289\n",
      "Evaluating after Epoch 9:\n",
      "Val loss is tensor([5.40837])\n",
      "\tTrain Epoch: 10 [0/2646 (0%)]\tLoss: 4.662844\n",
      "\tTrain Epoch: 10 [200/2646 (8%)]\tLoss: 5.069086\n",
      "\tTrain Epoch: 10 [400/2646 (15%)]\tLoss: 1.321552\n",
      "\tTrain Epoch: 10 [600/2646 (23%)]\tLoss: 3.126653\n",
      "\tTrain Epoch: 10 [800/2646 (30%)]\tLoss: 1.706841\n",
      "\tTrain Epoch: 10 [1000/2646 (38%)]\tLoss: 1.898558\n",
      "\tTrain Epoch: 10 [1200/2646 (45%)]\tLoss: 2.383604\n",
      "\tTrain Epoch: 10 [1400/2646 (53%)]\tLoss: 2.732483\n",
      "\tTrain Epoch: 10 [1600/2646 (60%)]\tLoss: 2.416741\n",
      "\tTrain Epoch: 10 [1800/2646 (68%)]\tLoss: 3.187337\n",
      "\tTrain Epoch: 10 [2000/2646 (76%)]\tLoss: 2.528903\n",
      "\tTrain Epoch: 10 [2200/2646 (83%)]\tLoss: 2.956549\n",
      "\tTrain Epoch: 10 [2400/2646 (91%)]\tLoss: 2.303646\n",
      "\tTrain Epoch: 10 [2600/2646 (98%)]\tLoss: 2.030042\n",
      "Evaluating after Epoch 10:\n",
      "Val loss is tensor([5.38012])\n",
      "\tTrain Epoch: 11 [0/2646 (0%)]\tLoss: 3.363230\n",
      "\tTrain Epoch: 11 [200/2646 (8%)]\tLoss: 2.909289\n",
      "\tTrain Epoch: 11 [400/2646 (15%)]\tLoss: 2.291738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 11 [600/2646 (23%)]\tLoss: 1.802421\n",
      "\tTrain Epoch: 11 [800/2646 (30%)]\tLoss: 2.088045\n",
      "\tTrain Epoch: 11 [1000/2646 (38%)]\tLoss: 2.584157\n",
      "\tTrain Epoch: 11 [1200/2646 (45%)]\tLoss: 2.761710\n",
      "\tTrain Epoch: 11 [1400/2646 (53%)]\tLoss: 4.422080\n",
      "\tTrain Epoch: 11 [1600/2646 (60%)]\tLoss: 3.029150\n",
      "\tTrain Epoch: 11 [1800/2646 (68%)]\tLoss: 6.265634\n",
      "\tTrain Epoch: 11 [2000/2646 (76%)]\tLoss: 5.016338\n",
      "\tTrain Epoch: 11 [2200/2646 (83%)]\tLoss: 2.080189\n",
      "\tTrain Epoch: 11 [2400/2646 (91%)]\tLoss: 2.056358\n",
      "\tTrain Epoch: 11 [2600/2646 (98%)]\tLoss: 2.215338\n",
      "Evaluating after Epoch 11:\n",
      "Val loss is tensor([5.57415])\n",
      "\tTrain Epoch: 12 [0/2646 (0%)]\tLoss: 1.982955\n",
      "\tTrain Epoch: 12 [200/2646 (8%)]\tLoss: 1.866649\n",
      "\tTrain Epoch: 12 [400/2646 (15%)]\tLoss: 1.966290\n",
      "\tTrain Epoch: 12 [600/2646 (23%)]\tLoss: 1.864289\n",
      "\tTrain Epoch: 12 [800/2646 (30%)]\tLoss: 1.106294\n",
      "\tTrain Epoch: 12 [1000/2646 (38%)]\tLoss: 2.143059\n",
      "\tTrain Epoch: 12 [1200/2646 (45%)]\tLoss: 3.379297\n",
      "\tTrain Epoch: 12 [1400/2646 (53%)]\tLoss: 2.364365\n",
      "\tTrain Epoch: 12 [1600/2646 (60%)]\tLoss: 2.356602\n",
      "\tTrain Epoch: 12 [1800/2646 (68%)]\tLoss: 2.735406\n",
      "\tTrain Epoch: 12 [2000/2646 (76%)]\tLoss: 2.546192\n",
      "\tTrain Epoch: 12 [2200/2646 (83%)]\tLoss: 2.653265\n",
      "\tTrain Epoch: 12 [2400/2646 (91%)]\tLoss: 4.664132\n",
      "\tTrain Epoch: 12 [2600/2646 (98%)]\tLoss: 2.355746\n",
      "Evaluating after Epoch 12:\n",
      "Val loss is tensor([5.74825])\n",
      "\tTrain Epoch: 13 [0/2646 (0%)]\tLoss: 2.985852\n",
      "\tTrain Epoch: 13 [200/2646 (8%)]\tLoss: 3.936519\n",
      "\tTrain Epoch: 13 [400/2646 (15%)]\tLoss: 3.278827\n",
      "\tTrain Epoch: 13 [600/2646 (23%)]\tLoss: 1.169698\n",
      "\tTrain Epoch: 13 [800/2646 (30%)]\tLoss: 4.356169\n",
      "\tTrain Epoch: 13 [1000/2646 (38%)]\tLoss: 2.242826\n",
      "\tTrain Epoch: 13 [1200/2646 (45%)]\tLoss: 5.298666\n",
      "\tTrain Epoch: 13 [1400/2646 (53%)]\tLoss: 2.193079\n",
      "\tTrain Epoch: 13 [1600/2646 (60%)]\tLoss: 1.501939\n",
      "\tTrain Epoch: 13 [1800/2646 (68%)]\tLoss: 2.353859\n",
      "\tTrain Epoch: 13 [2000/2646 (76%)]\tLoss: 1.969363\n",
      "\tTrain Epoch: 13 [2200/2646 (83%)]\tLoss: 1.508895\n",
      "\tTrain Epoch: 13 [2400/2646 (91%)]\tLoss: 2.733961\n",
      "\tTrain Epoch: 13 [2600/2646 (98%)]\tLoss: 1.828406\n",
      "Evaluating after Epoch 13:\n",
      "Val loss is tensor([5.74919])\n",
      "\tTrain Epoch: 14 [0/2646 (0%)]\tLoss: 2.464663\n",
      "\tTrain Epoch: 14 [200/2646 (8%)]\tLoss: 1.162609\n",
      "\tTrain Epoch: 14 [400/2646 (15%)]\tLoss: 3.268951\n",
      "\tTrain Epoch: 14 [600/2646 (23%)]\tLoss: 2.130078\n",
      "\tTrain Epoch: 14 [800/2646 (30%)]\tLoss: 2.783648\n",
      "\tTrain Epoch: 14 [1000/2646 (38%)]\tLoss: 3.240922\n",
      "\tTrain Epoch: 14 [1200/2646 (45%)]\tLoss: 1.999826\n",
      "\tTrain Epoch: 14 [1400/2646 (53%)]\tLoss: 2.064276\n",
      "\tTrain Epoch: 14 [1600/2646 (60%)]\tLoss: 7.243496\n",
      "\tTrain Epoch: 14 [1800/2646 (68%)]\tLoss: 2.226636\n",
      "\tTrain Epoch: 14 [2000/2646 (76%)]\tLoss: 1.633012\n",
      "\tTrain Epoch: 14 [2200/2646 (83%)]\tLoss: 5.034368\n",
      "\tTrain Epoch: 14 [2400/2646 (91%)]\tLoss: 1.516949\n",
      "\tTrain Epoch: 14 [2600/2646 (98%)]\tLoss: 2.894680\n",
      "Evaluating after Epoch 14:\n",
      "Val loss is tensor([6.13056])\n",
      "\tTrain Epoch: 15 [0/2646 (0%)]\tLoss: 4.252638\n",
      "\tTrain Epoch: 15 [200/2646 (8%)]\tLoss: 4.052194\n",
      "\tTrain Epoch: 15 [400/2646 (15%)]\tLoss: 1.417825\n",
      "\tTrain Epoch: 15 [600/2646 (23%)]\tLoss: 2.101847\n",
      "\tTrain Epoch: 15 [800/2646 (30%)]\tLoss: 2.599244\n",
      "\tTrain Epoch: 15 [1000/2646 (38%)]\tLoss: 1.686332\n",
      "\tTrain Epoch: 15 [1200/2646 (45%)]\tLoss: 4.092675\n",
      "\tTrain Epoch: 15 [1400/2646 (53%)]\tLoss: 1.496068\n",
      "\tTrain Epoch: 15 [1600/2646 (60%)]\tLoss: 2.077020\n",
      "\tTrain Epoch: 15 [1800/2646 (68%)]\tLoss: 2.010986\n",
      "\tTrain Epoch: 15 [2000/2646 (76%)]\tLoss: 3.380557\n",
      "\tTrain Epoch: 15 [2200/2646 (83%)]\tLoss: 2.200403\n",
      "\tTrain Epoch: 15 [2400/2646 (91%)]\tLoss: 3.018000\n",
      "\tTrain Epoch: 15 [2600/2646 (98%)]\tLoss: 0.967995\n",
      "Evaluating after Epoch 15:\n",
      "Val loss is tensor([6.12569])\n",
      "\tTrain Epoch: 16 [0/2646 (0%)]\tLoss: 2.022047\n",
      "\tTrain Epoch: 16 [200/2646 (8%)]\tLoss: 2.189264\n",
      "\tTrain Epoch: 16 [400/2646 (15%)]\tLoss: 3.138446\n",
      "\tTrain Epoch: 16 [600/2646 (23%)]\tLoss: 2.956487\n",
      "\tTrain Epoch: 16 [800/2646 (30%)]\tLoss: 6.043015\n",
      "\tTrain Epoch: 16 [1000/2646 (38%)]\tLoss: 2.282460\n",
      "\tTrain Epoch: 16 [1200/2646 (45%)]\tLoss: 2.239053\n",
      "\tTrain Epoch: 16 [1400/2646 (53%)]\tLoss: 2.120289\n",
      "\tTrain Epoch: 16 [1600/2646 (60%)]\tLoss: 2.880145\n",
      "\tTrain Epoch: 16 [1800/2646 (68%)]\tLoss: 4.219313\n",
      "\tTrain Epoch: 16 [2000/2646 (76%)]\tLoss: 0.926517\n",
      "\tTrain Epoch: 16 [2200/2646 (83%)]\tLoss: 5.366912\n",
      "\tTrain Epoch: 16 [2400/2646 (91%)]\tLoss: 1.766400\n",
      "\tTrain Epoch: 16 [2600/2646 (98%)]\tLoss: 1.315897\n",
      "Evaluating after Epoch 16:\n",
      "Val loss is tensor([6.15992])\n",
      "\tTrain Epoch: 17 [0/2646 (0%)]\tLoss: 1.781483\n",
      "\tTrain Epoch: 17 [200/2646 (8%)]\tLoss: 1.363918\n",
      "\tTrain Epoch: 17 [400/2646 (15%)]\tLoss: 4.561429\n",
      "\tTrain Epoch: 17 [600/2646 (23%)]\tLoss: 3.062470\n",
      "\tTrain Epoch: 17 [800/2646 (30%)]\tLoss: 2.067848\n",
      "\tTrain Epoch: 17 [1000/2646 (38%)]\tLoss: 1.645072\n",
      "\tTrain Epoch: 17 [1200/2646 (45%)]\tLoss: 3.051680\n",
      "\tTrain Epoch: 17 [1400/2646 (53%)]\tLoss: 1.173369\n",
      "\tTrain Epoch: 17 [1600/2646 (60%)]\tLoss: 1.161357\n",
      "\tTrain Epoch: 17 [1800/2646 (68%)]\tLoss: 3.381293\n",
      "\tTrain Epoch: 17 [2000/2646 (76%)]\tLoss: 0.664378\n",
      "\tTrain Epoch: 17 [2200/2646 (83%)]\tLoss: 1.973261\n",
      "\tTrain Epoch: 17 [2400/2646 (91%)]\tLoss: 1.759086\n",
      "\tTrain Epoch: 17 [2600/2646 (98%)]\tLoss: 1.676353\n",
      "Evaluating after Epoch 17:\n",
      "Val loss is tensor([6.38847])\n",
      "\tTrain Epoch: 18 [0/2646 (0%)]\tLoss: 1.948119\n",
      "\tTrain Epoch: 18 [200/2646 (8%)]\tLoss: 1.918705\n",
      "\tTrain Epoch: 18 [400/2646 (15%)]\tLoss: 1.322743\n",
      "\tTrain Epoch: 18 [600/2646 (23%)]\tLoss: 1.296038\n",
      "\tTrain Epoch: 18 [800/2646 (30%)]\tLoss: 1.522477\n",
      "\tTrain Epoch: 18 [1000/2646 (38%)]\tLoss: 1.574835\n",
      "\tTrain Epoch: 18 [1200/2646 (45%)]\tLoss: 1.727905\n",
      "\tTrain Epoch: 18 [1400/2646 (53%)]\tLoss: 2.898185\n",
      "\tTrain Epoch: 18 [1600/2646 (60%)]\tLoss: 1.217691\n",
      "\tTrain Epoch: 18 [1800/2646 (68%)]\tLoss: 0.951044\n",
      "\tTrain Epoch: 18 [2000/2646 (76%)]\tLoss: 1.935688\n",
      "\tTrain Epoch: 18 [2200/2646 (83%)]\tLoss: 2.565292\n",
      "\tTrain Epoch: 18 [2400/2646 (91%)]\tLoss: 1.351321\n",
      "\tTrain Epoch: 18 [2600/2646 (98%)]\tLoss: 1.593561\n",
      "Evaluating after Epoch 18:\n",
      "Val loss is tensor([6.38230])\n",
      "\tTrain Epoch: 19 [0/2646 (0%)]\tLoss: 1.296965\n",
      "\tTrain Epoch: 19 [200/2646 (8%)]\tLoss: 0.969962\n",
      "\tTrain Epoch: 19 [400/2646 (15%)]\tLoss: 2.184593\n",
      "\tTrain Epoch: 19 [600/2646 (23%)]\tLoss: 1.691256\n",
      "\tTrain Epoch: 19 [800/2646 (30%)]\tLoss: 1.806871\n",
      "\tTrain Epoch: 19 [1000/2646 (38%)]\tLoss: 2.278658\n",
      "\tTrain Epoch: 19 [1200/2646 (45%)]\tLoss: 1.001948\n",
      "\tTrain Epoch: 19 [1400/2646 (53%)]\tLoss: 1.920090\n",
      "\tTrain Epoch: 19 [1600/2646 (60%)]\tLoss: 1.247190\n",
      "\tTrain Epoch: 19 [1800/2646 (68%)]\tLoss: 1.390606\n",
      "\tTrain Epoch: 19 [2000/2646 (76%)]\tLoss: 2.338902\n",
      "\tTrain Epoch: 19 [2200/2646 (83%)]\tLoss: 2.009182\n",
      "\tTrain Epoch: 19 [2400/2646 (91%)]\tLoss: 3.732445\n",
      "\tTrain Epoch: 19 [2600/2646 (98%)]\tLoss: 1.294047\n",
      "Evaluating after Epoch 19:\n",
      "Val loss is tensor([6.62201])\n",
      "\tTrain Epoch: 20 [0/2646 (0%)]\tLoss: 2.732729\n",
      "\tTrain Epoch: 20 [200/2646 (8%)]\tLoss: 1.907125\n",
      "\tTrain Epoch: 20 [400/2646 (15%)]\tLoss: 2.149121\n",
      "\tTrain Epoch: 20 [600/2646 (23%)]\tLoss: 4.543168\n",
      "\tTrain Epoch: 20 [800/2646 (30%)]\tLoss: 1.247624\n",
      "\tTrain Epoch: 20 [1000/2646 (38%)]\tLoss: 1.225706\n",
      "\tTrain Epoch: 20 [1200/2646 (45%)]\tLoss: 1.405950\n",
      "\tTrain Epoch: 20 [1400/2646 (53%)]\tLoss: 1.397211\n",
      "\tTrain Epoch: 20 [1600/2646 (60%)]\tLoss: 1.542278\n",
      "\tTrain Epoch: 20 [1800/2646 (68%)]\tLoss: 2.282192\n",
      "\tTrain Epoch: 20 [2000/2646 (76%)]\tLoss: 1.772456\n",
      "\tTrain Epoch: 20 [2200/2646 (83%)]\tLoss: 1.339440\n",
      "\tTrain Epoch: 20 [2400/2646 (91%)]\tLoss: 1.779166\n",
      "\tTrain Epoch: 20 [2600/2646 (98%)]\tLoss: 1.848025\n",
      "Evaluating after Epoch 20:\n",
      "Val loss is tensor([6.53483])\n",
      "\tTrain Epoch: 21 [0/2646 (0%)]\tLoss: 1.297966\n",
      "\tTrain Epoch: 21 [200/2646 (8%)]\tLoss: 1.422842\n",
      "\tTrain Epoch: 21 [400/2646 (15%)]\tLoss: 1.757406\n",
      "\tTrain Epoch: 21 [600/2646 (23%)]\tLoss: 1.649754\n",
      "\tTrain Epoch: 21 [800/2646 (30%)]\tLoss: 1.387389\n",
      "\tTrain Epoch: 21 [1000/2646 (38%)]\tLoss: 2.300542\n",
      "\tTrain Epoch: 21 [1200/2646 (45%)]\tLoss: 4.269344\n",
      "\tTrain Epoch: 21 [1400/2646 (53%)]\tLoss: 1.434864\n",
      "\tTrain Epoch: 21 [1600/2646 (60%)]\tLoss: 1.898310\n",
      "\tTrain Epoch: 21 [1800/2646 (68%)]\tLoss: 1.884025\n",
      "\tTrain Epoch: 21 [2000/2646 (76%)]\tLoss: 2.920789\n",
      "\tTrain Epoch: 21 [2200/2646 (83%)]\tLoss: 3.369878\n",
      "\tTrain Epoch: 21 [2400/2646 (91%)]\tLoss: 3.554254\n",
      "\tTrain Epoch: 21 [2600/2646 (98%)]\tLoss: 0.769244\n",
      "Evaluating after Epoch 21:\n",
      "Val loss is tensor([6.67187])\n",
      "\tTrain Epoch: 22 [0/2646 (0%)]\tLoss: 1.906281\n",
      "\tTrain Epoch: 22 [200/2646 (8%)]\tLoss: 1.880559\n",
      "\tTrain Epoch: 22 [400/2646 (15%)]\tLoss: 1.451954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 22 [600/2646 (23%)]\tLoss: 1.169419\n",
      "\tTrain Epoch: 22 [800/2646 (30%)]\tLoss: 1.104732\n",
      "\tTrain Epoch: 22 [1000/2646 (38%)]\tLoss: 1.364640\n",
      "\tTrain Epoch: 22 [1200/2646 (45%)]\tLoss: 1.767231\n",
      "\tTrain Epoch: 22 [1400/2646 (53%)]\tLoss: 2.164535\n",
      "\tTrain Epoch: 22 [1600/2646 (60%)]\tLoss: 1.511129\n",
      "\tTrain Epoch: 22 [1800/2646 (68%)]\tLoss: 0.777685\n",
      "\tTrain Epoch: 22 [2000/2646 (76%)]\tLoss: 2.408559\n",
      "\tTrain Epoch: 22 [2200/2646 (83%)]\tLoss: 1.343196\n",
      "\tTrain Epoch: 22 [2400/2646 (91%)]\tLoss: 1.406842\n",
      "\tTrain Epoch: 22 [2600/2646 (98%)]\tLoss: 1.560818\n",
      "Evaluating after Epoch 22:\n",
      "Val loss is tensor([6.75005])\n",
      "\tTrain Epoch: 23 [0/2646 (0%)]\tLoss: 2.201466\n",
      "\tTrain Epoch: 23 [200/2646 (8%)]\tLoss: 1.377552\n",
      "\tTrain Epoch: 23 [400/2646 (15%)]\tLoss: 1.911509\n",
      "\tTrain Epoch: 23 [600/2646 (23%)]\tLoss: 1.315960\n",
      "\tTrain Epoch: 23 [800/2646 (30%)]\tLoss: 1.526100\n",
      "\tTrain Epoch: 23 [1000/2646 (38%)]\tLoss: 3.481951\n",
      "\tTrain Epoch: 23 [1200/2646 (45%)]\tLoss: 1.744251\n",
      "\tTrain Epoch: 23 [1400/2646 (53%)]\tLoss: 1.281623\n",
      "\tTrain Epoch: 23 [1600/2646 (60%)]\tLoss: 1.369275\n",
      "\tTrain Epoch: 23 [1800/2646 (68%)]\tLoss: 1.552676\n",
      "\tTrain Epoch: 23 [2000/2646 (76%)]\tLoss: 1.287148\n",
      "\tTrain Epoch: 23 [2200/2646 (83%)]\tLoss: 1.688876\n",
      "\tTrain Epoch: 23 [2400/2646 (91%)]\tLoss: 1.693098\n",
      "\tTrain Epoch: 23 [2600/2646 (98%)]\tLoss: 0.815877\n",
      "Evaluating after Epoch 23:\n",
      "Val loss is tensor([6.94487])\n",
      "\tTrain Epoch: 24 [0/2646 (0%)]\tLoss: 1.234699\n",
      "\tTrain Epoch: 24 [200/2646 (8%)]\tLoss: 1.741401\n",
      "\tTrain Epoch: 24 [400/2646 (15%)]\tLoss: 1.856820\n",
      "\tTrain Epoch: 24 [600/2646 (23%)]\tLoss: 2.109605\n",
      "\tTrain Epoch: 24 [800/2646 (30%)]\tLoss: 2.983731\n",
      "\tTrain Epoch: 24 [1000/2646 (38%)]\tLoss: 0.975910\n",
      "\tTrain Epoch: 24 [1200/2646 (45%)]\tLoss: 1.056494\n",
      "\tTrain Epoch: 24 [1400/2646 (53%)]\tLoss: 0.827635\n",
      "\tTrain Epoch: 24 [1600/2646 (60%)]\tLoss: 1.295226\n",
      "\tTrain Epoch: 24 [1800/2646 (68%)]\tLoss: 1.406816\n",
      "\tTrain Epoch: 24 [2000/2646 (76%)]\tLoss: 0.893904\n",
      "\tTrain Epoch: 24 [2200/2646 (83%)]\tLoss: 4.268097\n",
      "\tTrain Epoch: 24 [2400/2646 (91%)]\tLoss: 2.294899\n",
      "\tTrain Epoch: 24 [2600/2646 (98%)]\tLoss: 0.937643\n",
      "Evaluating after Epoch 24:\n",
      "Val loss is tensor([6.86899])\n",
      "\tTrain Epoch: 25 [0/2646 (0%)]\tLoss: 1.155775\n",
      "\tTrain Epoch: 25 [200/2646 (8%)]\tLoss: 1.265834\n",
      "\tTrain Epoch: 25 [400/2646 (15%)]\tLoss: 1.901387\n",
      "\tTrain Epoch: 25 [600/2646 (23%)]\tLoss: 1.857226\n",
      "\tTrain Epoch: 25 [800/2646 (30%)]\tLoss: 0.899801\n",
      "\tTrain Epoch: 25 [1000/2646 (38%)]\tLoss: 1.154424\n",
      "\tTrain Epoch: 25 [1200/2646 (45%)]\tLoss: 1.252674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1f95c5880619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Evaluate at the end of the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-355c01d407d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Run through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Calculate loss and take step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yolov3/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Augment images (inference and test only) https://github.com/ultralytics/yolov3/issues/931\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# height, width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yolov3/models.py\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0myolo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# run module directly, i.e. mtype = 'convolutional', 'upsample', 'maxpool', 'batchnorm2d' etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_loss_hist = []\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, optimizer, None, epoch)\n",
    "    \n",
    "    # Evaluate at the end of the epoch\n",
    "    print(\"Evaluating after Epoch {}:\".format(epoch))\n",
    "    model.training= False\n",
    "    val_loss = evaluate(valloader, model)\n",
    "    model.training=True\n",
    "    print(\"Val loss is {}\".format(val_loss.cpu().detach()))\n",
    "    min_val_loss = float('inf')\n",
    "    # If this is the best model so far, save it\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'conv_state_dict': conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            }, 'best_bounding_box.pt')\n",
    "    \n",
    "    # Save loss \n",
    "    val_loss_hist.append(val_loss)\n",
    "    #val_threat_score_hist.append(val_threat_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image = iter(valloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bounding_boxes(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import LabeledDataset\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map\n",
    "\n",
    "from model_loader import get_transform_task1, get_transform_task2, ModelLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='data')\n",
    "parser.add_argument('--testset', action='store_true')\n",
    "parser.add_argument('--verbose', action='store_true')\n",
    "opt = parser.parse_args()\n",
    "\n",
    "image_folder = opt.data_dir\n",
    "annotation_csv = f'{opt.data_dir}/annotation.csv'\n",
    "\n",
    "if opt.testset:\n",
    "    labeled_scene_index = np.arange(134, 148)\n",
    "else:\n",
    "    labeled_scene_index = np.arange(120, 134)\n",
    "\n",
    "# For bounding boxes task\n",
    "labeled_trainset_task1 = LabeledDataset(\n",
    "    image_folder=image_folder,\n",
    "    annotation_file=annotation_csv,\n",
    "    scene_index=labeled_scene_index,\n",
    "    transform=get_transform_task1(),\n",
    "    extra_info=False\n",
    "    )\n",
    "dataloader_task1 = torch.utils.data.DataLoader(\n",
    "    labeled_trainset_task1,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    "    )\n",
    "# For road map task\n",
    "labeled_trainset_task2 = LabeledDataset(\n",
    "    image_folder=image_folder,\n",
    "    annotation_file=annotation_csv,\n",
    "    scene_index=labeled_scene_index,\n",
    "    transform=get_transform_task2(),\n",
    "    extra_info=False\n",
    "    )\n",
    "dataloader_task2 = torch.utils.data.DataLoader(\n",
    "    labeled_trainset_task2,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    "    )\n",
    "\n",
    "model_loader = ModelLoader()\n",
    "\n",
    "total = 0\n",
    "total_ats_bounding_boxes = 0\n",
    "total_ts_road_map = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader_task1):\n",
    "        total += 1\n",
    "        sample, target, road_image = data\n",
    "        sample = sample.cuda()\n",
    "\n",
    "        predicted_bounding_boxes = model_loader.get_bounding_boxes(sample)[0].cpu()\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes, target['bounding_box'][0])\n",
    "        total_ats_bounding_boxes += ats_bounding_boxes\n",
    "\n",
    "        if opt.verbose:\n",
    "            print(f'{i} - Bounding Box Score: {ats_bounding_boxes:.4}')\n",
    "\n",
    "    for i, data in enumerate(dataloader_task2):\n",
    "        sample, target, road_image = data\n",
    "        sample = sample.cuda()\n",
    "\n",
    "        predicted_road_map = model_loader.get_binary_road_map(sample).cpu()\n",
    "        ts_road_map = compute_ts_road_map(predicted_road_map, road_image)\n",
    "        total_ts_road_map += ts_road_map\n",
    "\n",
    "        if opt.verbose:\n",
    "            print(f'{i} - Road Map Score: {ts_road_map:.4}')\n",
    "\n",
    "print(f'{model_loader.team_name} - {model_loader.round_number} - Bounding Box Score: {total_ats_bounding_boxes / total:.4} - Road Map Score: {total_ts_road_map / total:.4}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(134, 135)\n",
    "labeled_scene_index_test = labeled_scene_index[0]\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "labeled_testset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "testloader = torch.utils.data.DataLoader(labeled_testset, batch_size=1, shuffle=True, num_workers=2, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image = iter(testloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 2, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['bounding_box'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[0].view(1, 18, 256, 306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 256, 306])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(samples):\n",
    "        # samples is a cuda tensor with size [batch_size, 6, 3, 256, 306]\n",
    "        # You need to return a tuple with size 'batch_size' and each element is a cuda tensor [N, 2, 4]\n",
    "        # where N is the number of object\n",
    "        b= []\n",
    "        model.eval()\n",
    "        conv.eval()\n",
    "        batch_size = samples.shape[0]\n",
    "        sample = samples.view(batch_size, 18, 256, 306).to(device)\n",
    "        sample = conv(sample)\n",
    "        pred = model(sample)\n",
    "        conf_thres = 0.08\n",
    "        iou_thres = 0.6\n",
    "        pred = non_max_suppression(pred[0], conf_thres, iou_thres, multi_label=False, classes=None)\n",
    "        for i, p in enumerate(pred):\n",
    "            pred[i][:, 0] = p[:, 0] * 80/256 - 40\n",
    "            pred[i][:, 1] = p[:, 1] * 80/306 - 40\n",
    "            pred[i][:, 2] = p[:, 2] * 80/256 - 40\n",
    "            pred[i][:, 3] = p[:, 3] * 80/306 - 40\n",
    "        box_num = pred[0].size()[0]\n",
    "        bbox = []\n",
    "        for i in range(box_num):\n",
    "            bbox.append(torch.tensor([[max(pred[0][i][2], pred[0][i][0]),  max(pred[0][i][2], pred[0][i][0]), min(pred[0][i][2], pred[0][i][0]),  min(pred[0][i][2], pred[0][i][0])],\n",
    "            [min(pred[0][i][1],pred[0][i][3]),   max(pred[0][i][1],pred[0][i][3]), max(pred[0][i][1],pred[0][i][3]), min(pred[0][i][1],pred[0][i][3])]]))\n",
    "        bbox = torch.stack(bbox)\n",
    "        bbox[bbox>40] = 40\n",
    "        bbox[bbox<-40] = -40\n",
    "        b.append(bbox)\n",
    "        return tuple(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def convert_map_to_lane_map(ego_map, binary_lane):\n",
    "    mask = (ego_map[0,:,:] == ego_map[1,:,:]) * (ego_map[1,:,:] == ego_map[2,:,:]) + (ego_map[0,:,:] == 250 / 255)\n",
    "\n",
    "    if binary_lane:\n",
    "        return (~ mask)\n",
    "    return ego_map * (~ mask.view(1, ego_map.shape[1], ego_map.shape[2]))\n",
    "\n",
    "def convert_map_to_road_map(ego_map):\n",
    "    mask = (ego_map[0,:,:] == 1) * (ego_map[1,:,:] == 1) * (ego_map[2,:,:] == 1)\n",
    "\n",
    "    return (~mask)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def draw_box(ax, corners, color):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    \n",
    "    # the corners are in meter and time 10 will convert them in pixels\n",
    "    # Add 400, since the center of the image is at pixel (400, 400)\n",
    "    # The negative sign is because the y axis is reversed for matplotlib\n",
    "    ax.plot(point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400, color=color)\n",
    "\n",
    "def compute_ats_bounding_boxes(boxes1, boxes2):\n",
    "    num_boxes1 = boxes1.size(0)\n",
    "    num_boxes2 = boxes2.size(0)\n",
    "\n",
    "    boxes1_max_x = boxes1[:, 0].max(dim=1)[0]\n",
    "    boxes1_min_x = boxes1[:, 0].min(dim=1)[0]\n",
    "    boxes1_max_y = boxes1[:, 1].max(dim=1)[0]\n",
    "    boxes1_min_y = boxes1[:, 1].min(dim=1)[0]\n",
    "\n",
    "    boxes2_max_x = boxes2[:, 0].max(dim=1)[0]\n",
    "    boxes2_min_x = boxes2[:, 0].min(dim=1)[0]\n",
    "    boxes2_max_y = boxes2[:, 1].max(dim=1)[0]\n",
    "    boxes2_min_y = boxes2[:, 1].min(dim=1)[0]\n",
    "\n",
    "    condition1_matrix = (boxes1_max_x.unsqueeze(1) > boxes2_min_x.unsqueeze(0))\n",
    "    condition2_matrix = (boxes1_min_x.unsqueeze(1) < boxes2_max_x.unsqueeze(0))\n",
    "    condition3_matrix = (boxes1_max_y.unsqueeze(1) > boxes2_min_y.unsqueeze(0))\n",
    "    condition4_matrix = (boxes1_min_y.unsqueeze(1) < boxes2_max_y.unsqueeze(0))\n",
    "    condition_matrix = condition1_matrix * condition2_matrix * condition3_matrix * condition4_matrix\n",
    "\n",
    "    iou_matrix = torch.zeros(num_boxes1, num_boxes2)\n",
    "    for i in range(num_boxes1):\n",
    "        for j in range(num_boxes2):\n",
    "            if condition_matrix[i][j]:\n",
    "                iou_matrix[i][j] = compute_iou(boxes1[i], boxes2[j])\n",
    "\n",
    "    iou_max = iou_matrix.max(dim=0)[0]\n",
    "\n",
    "    iou_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    total_threat_score = 0\n",
    "    total_weight = 0\n",
    "    for threshold in iou_thresholds:\n",
    "        tp = (iou_max > threshold).sum()\n",
    "        threat_score = tp * 1.0 / (num_boxes1 + num_boxes2 - tp)\n",
    "        total_threat_score += 1.0 / threshold * threat_score\n",
    "        total_weight += 1.0 / threshold\n",
    "\n",
    "    average_threat_score = total_threat_score / total_weight\n",
    "    \n",
    "    return average_threat_score\n",
    "\n",
    "def compute_ts_road_map(road_map1, road_map2):\n",
    "    tp = (road_map1 * road_map2).sum()\n",
    "\n",
    "    return tp * 1.0 / (road_map1.sum() + road_map2.sum() - tp)\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    a = Polygon(torch.t(box1)).convex_hull\n",
    "    b = Polygon(torch.t(box2)).convex_hull\n",
    "    \n",
    "    return a.intersection(b).area / a.union(b).area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Bounding Box Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_ats_bounding_boxes = 0\n",
    "total_ts_road_map = 0\n",
    "with torch.no_grad():\n",
    "    total += 1\n",
    "    predicted_bounding_boxes = get_bounding_boxes(sample)[0].cpu()\n",
    "    ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes, target['bounding_box'])\n",
    "    total_ats_bounding_boxes += ats_bounding_boxes\n",
    "\n",
    "\n",
    "print(f' - Bounding Box Score: {total_ats_bounding_boxes / total:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(313.64111, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, target, road_image = iter(valloader).next()\n",
    "model.eval()\n",
    "sample = torch.stack(sample).to(device)\n",
    "\n",
    "# Make input the correct shape\n",
    "batch_size = sample.shape[0]\n",
    "sample = sample.view(batch_size, 18, 256, 306)\n",
    "sample = conv(sample)\n",
    "\n",
    "# Run through model\n",
    "optimizer.zero_grad()\n",
    "output = model(sample)\n",
    "output[0][1, :, 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply non-max supression\n",
    "#Returns list of length batch_size, with each list element being a tensor of size nx6 (x1, y1, x2, y2, conf, cls) \n",
    "conf_thres = 0.07\n",
    "iou_thres = 0.6\n",
    "output = non_max_suppression(output[0], conf_thres, iou_thres,\n",
    "                                   multi_label=False, classes=None, )\n",
    "# Rescale from 256x306 to 80x80 (from -40 to 40)\n",
    "for i, preds in enumerate(output):\n",
    "    output[i][:, 0] = preds[:, 0] * 80/256 - 40\n",
    "    output[i][:, 1] = preds[:, 1] * 80/306 - 40\n",
    "    output[i][:, 2] = preds[:, 2] * 80/256 - 40\n",
    "    output[i][:, 3] = preds[:, 3] * 80/306 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 6])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store bounding boxes in the correct format\n",
    "bounding_boxes = torch.zeros((output[0].shape[0], 2, 4))\n",
    "for i in range(output[0].shape[0]):\n",
    "    # Get four corners\n",
    "    bounding_boxes[i, :, :] = torch.tensor([[max(output[0][i][2], output[0][i][0]),  max(output[0][i][2], output[0][i][0]), min(output[0][i][2], output[0][i][0]),  min(output[0][i][2], output[0][i][0])],\n",
    "                                            [min(output[0][i][1],output[0][i][3]),   max(output[0][i][1],output[0][i][3]), max(output[0][i][1],output[0][i][3]), min(output[0][i][1],output[0][i][3])]])\n",
    "# Truncate corners that are out of range\n",
    "bounding_boxes[bounding_boxes>40] = 40\n",
    "bounding_boxes[bounding_boxes<-40] = -40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot original\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for dimension 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-359-5526d8f5161d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounding_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# You can check the implementation of the draw box to understand how it works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdraw_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for dimension 0 with size 7"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAIcCAYAAAATyGH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAewgAAHsIBbtB1PgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3debxdVX338c8vCRkIUwAxSKJCAAlEARkEAZkFFYdqRdTWB4c6lra2FYc+itZqK1qrj7VVq63WOtUW61ARAXEARUABmUOYvGGeh0ASkqznj7WOd+fkjPfec8+593zer9d+nb3PXnvtfc++w/euvfbakVJCkiRJMKPfByBJkjQoDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBVDGYwi4skR8bGIuCYiVkXEfRFxUUT8ZURs3u/jkyRJ/REppX4fw6SKiBcAXwG2blLkOuD5KaUbJ++oJEnSIBiqYBQRewM/BzYHHgH+FjgPmAecBPxRKXotcEBK6ZF+HKckSeqPYQtG5wFHAOuA56SUflG3/h3A6WXxtJTSX0/uEUqSpH4ammAUEQcAF5XFz6aU3tygzAzgSmApcD/wxJTS45N3lJIkqZ+GqfP1Syrz/9aoQEppA/DvZXEBuXVJkiQNiWEKRoeV11XAr1qU+0ll/tDeHY4kSRo0s/p9AJNoaXldkVJa16LctQ226UhELGpTZDawB3AXcDewvpv6JUkaYjOBJ5T5K1JKa3qxk6EIRhExF9i+LK5sVTaldH9ErALmA4u73NXIGA5PkiR15wDgkl5UPCyX0raszHdyC/6q8rpFD45FkiQNqKFoMQLmVubXdlC+1jw3r8v9tGth2gm4EOCiiy5ixx137LJ6SRpMixd328A+diMjNs4Po9tvv50DDzywtnh3r/YzLMFodWV+dgfl55TXx7rZSUqp5WW6iPjd/I477siiRe26JEmS6vm7U/Swj+6wXEp7uDLfyeWx+eXVka8lqY3qP33SVDcUwSiltBq4pyy2/FcjIhYwGoxsr5WkFiY7FA3LoMTqn6EIRsU15XXXiGh1CXGPBttIkvrMUKTJMEzB6PzyOh/Yr0W5wyvzF/TucCRpapvM1iJDkSbLMAWj/6nMv7ZRgfKstNeUxQeA83p9UJIkaXAMTTBKKV0E/Kwsvj4iDm5Q7C8YHe36kz5AVpIas7VI09Ww3K5f86fky2PzgB9GxIfJrULzgJOAN5Zyy4G/78sRSpKkvhmqYJRSujQiXgH8B7AV8OEGxZYDL0gpPdxgnSRpEtlapMk2NJfSalJK3wWeAfwDOQQ9Su5PdAnwTmDflNKK/h2hJEnql6FqMapJKd0C/HmZJEkDyNYi9cPQtRhJkiQ1YzCSJHVlMu5Is7VI/WIwkiRJKgxGkiRJhcFIkjRQvIymfjIYSZI6NpkjXkv9YDCSJEkqDEaSpI7YWqRhYDCSJEkqDEaSJEmFwUiSNDC8I039ZjCSJLVl/yINC4ORJElSYTCSJEkqDEaSJEmFwUiSNBDseK1BYDCSJLVkx2sNE4ORJElSYTCSJEkqDEaSJEmFwUiaRPV9NSLid1Oj9VK/+T2pYTOr3wcwrBYvXrzJe7U7MiKClNJGr9X19Wrlul2nidfJH5FmZVqFI8+hJE0Og9EAqf5BbPRHstUf3bGum0zVsFfTKPRV3xtPKJwMk/XZGpYkaXIYjDRpWrWItHtvPMGvGrAarWtVZ6PANij6HQqlieT3sgaFwUjT3nhC1aCFoXrV4/MPi7pV//1d30o7md//nXYdkHrNYCRNE4YkjVenrbT92L/f05os3pUmSZJUGIykaag6BIAkqXMGI2kaMxxJUncMRtI0ZziSpM4ZjKQh4KU1SeqMd6VJQ8SxjzQV+D2qfjIYSUPGcKRB5PekBoWX0qQh5GU1SWrMFiNpSNly1FyrR8j4ufWGn6sGhcFIGmKOlt2dVs/7a8dQJU0NBiNJwPCFpMm+nNguVDV7Nlkvz0Wruvv1SJBh+N7TYDMYSdrEdHmQ51TqS9XsWCfja6gPZVP9vEvjYTCS1NRU+GM5lcLPoKr/DPv5mdpqpH4zGEnqyCC0IhmChkMtHNWfbwOTJoPBqE9GRkZYtGhRyzKt+htU/6vyj4Um02T9sfL7erg1Ov8T/T3R7HvXVqvhZjAaYK1+MKvrpvoPcLUlotkvpGYhsdUvynbrNTG6+Yxb/SGSJlur77vJ7NtV/bmob5nttvVsEFp2pzqDkfquk5DX7fudru+Wf8DHx89PGtXqTsXqe2NpPRuEn7WpGs4MRlIXOm3NkiRNTT4SRBqnlNKU/c9IkrQxW4ykCdKon4AkaWqxxUjqAVuQJGlqssVI6hFbkCRp6rHFSJoEtiBJ0tRgMJImieFIkgafwUiaRLVwZEiSpMFkMJImmaFIkgaXwUjqEwOSJA0eg5HUR4YjSRosBiNJkqTCYCRJklQ4wGOfLF68eKPllNImgwBW36tdcmk2UGCzSzL12zcrU62/0bG020f99p3sq9H7Y91+olS/7lbHU/85eUlMkqYHg9GAaBREqu+1Gzl5Itc3K9uqjok41tr74/1aJkq74+nma5YkTQ1eSpMkSSoMRpIkSYXBSJIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkSYXBSJIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkSYXBSJIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkSYXBSJIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZKKngajiHhmRLwnIs6MiJGIWBMRj0TE8oj4YkQc1mV9x0fEGRGxstS1siwf30UdsyLiTRHx04i4OyIei4gVEfGZiNiz+69SkiRNF5FS6k3FET8BntNB0S8Db0gprW1RVwCfAd7Yop7PAW9OLb6giNgO+F/gWU2KrAHemlL617ZHPQYRsQgY6UXdkiQNkonOFytXrmTx4sW1xcUppZUTuoOily1GO5XX24BPAr8PHAgcDPw5cGtZ/4fAF9vU9TeMhqJLgVeWul5ZlinrP9isgoiYCZzBaCg6A3heWf4T4C5gDvC5iDiu3RcnSZKmoZRSTybge8CJwMwm67cHrgNSmQ5rUm5X4PFS5mJgXt36zcv7qZRb0qSekyv7+nST/TxY1i8HZvXgM1lUOQYnJycnJ6dpO020kZGRav2LJvpvdG3qWYtRSumElNJ/ppTWN1l/D/AXlbd+v0lVbwdmlflTUkqP1dXzKHBKWZwF/FmTet5RXu+vzFfrWQH8bVncDXhxk3okSdI01e+70n5cmV9Sv7L0LaoFlGtTShc2qqS8f11ZfEnZrlrPbkCtY/U3Sphq5IuV+Ze2PHJJkjTt9DsYza7Mb2iwfmdG+yr9pE1dtfWLgKfWrTusQblNpJTuIF9GAzi0zf4kSdI0M6t9kZ46vDJ/bYP1S9usp8n6pcBN46hnd2BxRMxPKa1qU/53yl1nrSzstC5JkjT5+haMImIG8K7KW//ZoNjiyny72/Kqt8Evrls3lnqC3Pp0XYuyrY5BkiRNMf28lPZ28i33AN9KKV3SoMyWlflH2tRXbdnZokf1SJKkaawvLUYRcTjwd2XxLuAtTYrOrcw3HQCyWFOZn9ejetqpb6mqt5A8tIAkSRpAkx6MImIv4Ftl32uAE1NKdzYpvroyP7tJmZo5lfnH6tbV17Oa5lrV01JqMwpn3c1ykiRpwEzqpbSI2Bn4IbAAWA+8MqXU6m6zhyvz7S5rza/M118um6h6JEnSNDZpwSgingScAzyJPGrl61JK32qzWbUFpt0dX9XLWPWdoMdST6J9R21JkjSNTEowiojtgbOBXcpbp6SU/r2DTa+uzO/Rpmx1/TUTUM9IN7fqS5Kkqa/nwSgitgbOYnTk6XellD7d4eY3kR9CCxuPedTIc8rrrcDNdevOr8w3rSciFpLHMAK4oLNDlCRJ00VPg1FEbA78L/DM8taHUkof6XT7lJ+8+u2yuEdEHNRkPwcx2tLz7bJdtZ7ljLYinViOq5GTK/PtLvNJkqRppmfBKCJmk8PFIeWtT6aU/u8YqvoEsK7MfyoiNrqFvix/qiyuK+Ub+Vh53RY4vcHxLgHeXRZvwGAkSdLQ6eXt+l8DnlvmfwR8ISKWtSi/trTsbCSltDwiPkYeJXt/4IKI+Ag5vCwB3gnsW4p/NKV0fZP6vwS8jhzU3lYum/0LcD95oMn3AluRn9l2SkppXZN6JEnSNBV1V50mruKIbiu+JaX01CZ1zSCHmNe12P4LwBtTSo0eRlurZ3vg+8ABTYqsBf44pfQvHR1xl8qz1HxsiCRp2pvofLFy5UoWL/7dDeiL240dOFb9fCRIx1JKG1JKrwdeQO5zdBs5xNxWlp+fUnpDq1BU6rkHeDbwVnKH7HvJgz3eSA5ez+xVKJIkSYOvZy1G2pQtRpKkYWGLkSRJ0hRnMJIkSSoMRpIkSUUvb9eXNMmevTvsvhCuWgm/ugk2DFgXwkXbwmF7QEpwxsWw1kExJA0Yg5H67sg9YYu58Ivr4Z6H+3ccS3eCp+0I19wK193ev+MYq//zHPjim0aXH1gF510N51wF51wJy/vwNW2zeT6/Ry+DY/aCpz1pdN2Pr4YjPzT5xyRJrRiMhtThS+GvXgzrNuQ/nj+4HK6YxPvltpib/2B+8Pdh76fk9zZsgAtXwPcuzVOvj2fOZnD4HnDCvvCCfWGXHfL7ax6HEz6Ww8RUcvJhGy9vMx9+74A8Aay8L39N51wJ514Fdzww8ccwZzM4ZHc4ei84ZhnstzPMbHLB/og9YecnwE13T/xxSOqf9JUy89XIr68asKbrNrxdfxINyu36m8+Bu/85v1bdeh+c9Rv4wW/g7CvggUcndr9PXwzH7w3HPwMOfRrMbhPLf3vPaEg672pY/fj4j2HhNvD8vXMYOvbpOaA18tWfw6s7fdTxgPj1h2Dfp3Ze/qqVo0HpJ9fCw491v88ZAfs8JYegY5bl8zpvdufb7/seuOyW7vcraTBtPgdW/Wvdmyc+ArPmj7vuybpd3xajIbTPUzYNRQA7bQuvOyJP60vrzQ8uz0HpVzflfiHd2Gbz/MeyFoZ22ra77Z+8Pbz12Dw9uib/Af/epfC/l8Ft93dWRwQ886m5ReiEfeCAJZ1tt+34f4YH3l6L8vSnx8O69XDRDSUoXQUXXg+Pr2+83ZInjgaho/aEbbeY3OOWNLiet3eDN287E578+5N+LGNlMBpCM6J9mZkz8iWRQ3aHD74c7n4IfnhFDkk//A3c9dCm20TkSyfHPyOHoYN2bX4ZpVubz4EX7ZcngF/flAPS9y6Fi2/cOLTNn5P/aJ+wL7xgH9hxwcQcw1Tzbz/J5+SYZbnTcyuzZuaO28/eHd73Uli1Gn56XQ5KV4zAE7aEo8rlsads391xPLYWflbq+vCJeV+SNKgMRurIE7aCVx+SJ8gtSD+4HH59M2w1L//BfO7Tc7nx+I/zcxjbeYfW5Z65c57e+3tw54Nw9a35/Tmzcjibs1ln+9uwAX6xAh5fl/u8TCfnXJkvCULuVH7Mstz358g9c/+jVubPzf/5Nfzvr431G+CSG0dbn35xfe63BfA3L+++PkmaTAYjcf8qOOi00ZaeI5a27yey38556tbNd8OZ5fLckh3g43+w8fq/+mbuW7R0p9zic8K+OSi1anl64tZ56tSDj+b9f+/SHO7ueRj+/PnTLxhVXXd7nj59dv4s99t5NCgdsRRmjLNl79rbRvsr/fia/BlL0lRkMBIp5Vu5l98O/+8smLsZPGeP0b5BS3cae90bNsBZV4z2VareMv5HRzbf7ppb8/TR78GC+flY/vDQsbVg1Py/s+Bbl8D51+U+NcNq/QZYtSZPj66Fibj9olbXo2sdm0jS1GYw0iY2pPzHszaNV62esYSRPZ60ccvReLzmUNhhK9hpQW61uu+R8dU3lSzadrSF6Jhl+e68iVRrQTz1hHzZ7ILleUiAc67Ml10n4vtIkiaDwUhAvtOodintyKW5j8lEmDFjNNgAXH/HaOtRozvj5swa7Th9wr75uCbKNvPhpIPztH5D7vtSGw6gg/7oU86hT8udqesHVuzE2nX58zn3KrjhztyP7PClOVi160c2Z7PcUfuoveBDJ44ONHnuVe2HaJCkfvPXlNh2C1jx8e62ueyWHHAu/20eC+jovXLn63a3bu+2ME+nHNd4/W/+FuZ2MQ7OZbeMhptrb8u3j9cGbGzV72jmjBwcDn0a/N1Jne9vKnnLMd2Vv+yW0Vaen12bL49VfebcfJfbM56cw9bRy+A5T2sfousHmqxauA3gOEaSBojBSB2575GNb9e/vW7U5M+fl4cBOGDJaMvTgbt036m3XSh6bG3+4/29S+H7l8HIvRuv/9YleYqA/Xcu4xft231H8eP3hu237O8jSnrt5rtHO0z/6Oo8JEM7KcHlt+Tp77+fW4AO2nX0Mt2BS7q7Hf8tR+eALUmDwmA0hDp5sOiGDXDRjaOXvS6+of12GxL8ckWePnBGbj06tjLA41j7tay8b7RV6EdX5XDUTkp5fKOLb4T3/zc8aQE8f58y4vWyxpfx6t2/amzH2y+PrG69/t6HcwCqhaEb7xr/Pteug59em6f3/dfoJbfaAJB7tum4f85V4z8GSZpIBqMhdPktOVzU35J/xwMbPxLk3nF2Tr7vEfjGhXmqXYI5/hl5OmR32KzJd18tlNXC0OUTcKnltvtzq9bnz8t33R2x52g/pmYDFk61DsNf+Xl+cn1NdWDFc67Ml8p6/QSghx6D7/46TwA7bjP6ANljlm06+vkvV/T2eCSpWz4rbRJVn5U2MjLCokWLxlPXRsspJSKC2vlstL667VF7wbmfPJKzzzmPY//gw7Dj8bBgb4gZDbev30/9fLe2mgf/eHK+Bb/q5M/CmZc1Hlm7V5YthvP+Kl86q4pXT94xTJRjluXBHK+6deOBFQfF10/JfdEWlAEmn/S2TS/LSpq69n4KXPbhujefdyks2GfcdfusNLXUKNBW32sVeKvrjj268/q73U/HUoKHroV5C/niq/r0/I60IU9r7snP9Vn0ItKrtuvLoVSDZ/W9Vp91bX1EcM6Vo+9Xg3Kz0Fwt22hdNXSPNQjXnPSpcW0uacA1HMdsRhd31AwAg5H6LwK2XtrnY5iRp3kLYclr+3oo7UJvq22alRtLaO6m/onQSfhrpL4Vc7zbdxP+JiIsStPJ8tthz1Pz/NVXlU6EW45zELpJZjCSNBA6DX9j2bab7bsNf3ZHmBoatZqO9dy1CvGtgnJ9kK5v0Z0OIXv9hvzUAgC2nprPWTIYSZKmvYnsAjCekN6qRdiQPRjG+ehISZKk6cNgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKlwHCNJ0sBpNOhh/TMha+U0mKbquEwGI0lSX7QLNfXra8uGIfWSwUiSNC4GFU0nBiNJkuFGKgxGkjQNGXSksTEYSdIAMthI/WEwkqRxqoUY75qSpj6DkaQpa9BCh3dNSVOfwUhS3xkkJA0Kg5GkcWsUbOoH55OkqcBgJA2RRqMJ93JfkjTV+Kw0aZpy1GBJ6p4tRtIU1egZUq3KSJLaMxhNEfW3AI+1XKf1tNoeRh8O2Mn+quXHUl/1vfr5RupvlW4WGupvrZ7KIWIqH7skDRKDUZ8sXry46206/ePX7YMZx6JaRyf1dXNM7Vo+xvrgyWblDBWSpBr7GEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSNAFSSv0+BEkTwGAkSRMgIvp9CJImgMFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqSib8EoIk6PiFSZjuhgm+Mj4oyIWBkRa8rrGRFxfBf7nRURb4qIn0bE3RHxWESsiIjPRMSe4/qiJEnSlNaXYBQRewNv76J8RMRngTOB3wN2AmaX198DzoyIz0ZEtKlnO+B84DPAYcD2wFxgCfAm4NcR8bruvyJJ/XbIJG8naXqa9GAUETOAfwFmAXd1uNnfAG8s85cCrwQOLK+XlvffCHywxX5nAmcAzypvnQE8ryz/STmWOcDnIuK4Do9L0gA4jfwfz6ldbndq2e60CT8iSVNWSmlSJ+DPgARcA3y4zCfgiCbldwUeL2UuBubVrd+8vJ9KuSVN6jm5sq9PN9nPg2X9cmBWD772RZVjcHJymoDpEEipMp3a4Xan1m13yAB8LU5O02maaCMjI9X6F0303+jaNKktRhGxmNFWnbcAazvY7O3k1iWAU1JKj1VXppQeBU4pi7PIwauRd5TX+yvz1XpWAH9bFncDXtzBsUnqswuAd1aWP0L7lqNTS7mad5Z6JGmyL6X9E7AF8KWU0o/bFS59hmoB5dqU0oWNypX3ryuLL6nvaxQRuwG1jtXfKGGqkS9W5l/a7vgkDYbT6TwcNQpFp/fouCRNPZMWjCLiROAE4D4atNg0sTO5gzXAT9qUra1fBDy1bt1hDcptIqV0B/kyGsChnR2ipEHQSTgyFElqZ1KCUURsA3yyLL4zpXR3h5surcxf26Zsdf3SunVjqWdxRMxvU1bSAGkVjgxFkjoxq32RCXE6sBD4OfCFLrZbXJlf2absSJPtxlpPkFufrmtRdiMRsahNkYWd1iVpbGph5yOV11OB7SplDEVS76SvlJmvll4tr0p9O5ax6HkwiohDgTcA64A3p3J7Voe2rMw/0qbsqsr8Fj2qp52R9kUk9Vp9ODIUSZNj8zkN3ly3CmZNnQswPb2UFhGzgc+RW1/+IaV0RZdVzK3Mt7uDbU1lfl6P6pE0RZwO3Fv33r0YiqReet7eDd687cxJP47x6HWL0XvI/Xt+C3xgDNuvrszPblO2mlMfq1tXX89qmmtVTzv1l/DqLSSPuSSpx+ovn1GWT8VwJKm5ngWjiNgDeHdZPCWltKpV+SYersy3u6xVbaerv1xWX0+rYNSqnpZSSi37L7V5YomkCVLf0fpeRkNS7X3DkaRGetli9HZy68yNwOYRcVKDMssq80dFRK1z8ndLkKoGjXYdm6utNfV9ferruaeDehLtO2pLGjDN7j6rvm84ktRML4NR7ZLULsDXOij/3sr8zuRO0FdX3tujzfbV9dfUrauv57IO6hkZYyuXpD5pdUt+o7vVqu9LEvThIbJdugm4rcwf3qbsc8rrrcDNdevOr8w3rae0WO1eFn1CgDSFdDJOUTcjZEsaTj0LRimlk1NK0Wpi4w7ZR1bW3VzqSMC3y/o9IuKgRvsq79daer5dPyRASmk5o61IJ0bE5k0O++TK/Lc6/Vol9Vc3gzcajiS1MugtRgCfII+BBPCpiNjoFvqy/KmyuK6Ub+Rj5XVbGvzOjIgljHYWvwGDkTQlHEL3I1o3CkeHTPBxSZqaBj4YldaeWqjZH7ggIl4REftHxCvIl7z2L+s/mlK6vklVX2L08tjbIuK/IuK4iDgwIv6YPCr3VsAG8l1065rUI2mAXAC8v8x3M3hjNRy9H6+dS8om65Eg4/VXwA7A64B9ga83KPMF4P82qyCltD4iXgJ8HzgAeFmZqtYCf5xSmlqjUUlD7gPAOXQfbk4v2xiKpImx4s4Gb26566Qfx3gMfIsRQEppQ0rp9cALyH2ObiOHmNvK8vNTSm9IKW1oU889wLOBt5I7ZN9LHtPoRuBfgGemlP6lZ1+IpJ4Za7gxFEkTZ22jay0z2o3PPFiiu0eXaTzKQ2Z9npokaVqaOQN23zHPX33VVXlmy91hxvgvUK1cuZLFi383ZOHidoMqj9VUuZQmSZIG3PoNcM2tZWHrPft6LGNlMJKkMZi7GXz0VfDs3eDme+DsK+DsK+GGRn0sxmnWTHjWEjj26XDknjBnFvzDmfCNCyd+X9Kw81LaJPJSmjR9/Pnz4e9fven7N9+dA9LZV8C5V8F9XT1xcdTSneCYZXDsMjhiKWw5b9MyT3ob3P7A2OqXem2i84WX0iRpgB24pPH7T30C/NGRedqwAS69ZbQ16YLlsObxxts9ceschGrTom3bH8MBS+A7vxr71yBpUwYjSRqD6KDMjBmw3855eteL4LG18LPrclD6+fWw9Tw4urQKPePJvTkGSd0xGEnSBFi3PvcFamXebHju0/M0Fhs25LAlqXcMRn0yMjLCokWL+rb/iBjX9d9221fXR+T/axuVH+9xTPTxTJTa/upfq/uvHUN1WVPXh78N//nL3Ppz7NPh8D1g/tzx1Tly7+hluHOvgvPfN3ortKTeMBgNqfGGgnbbV9e3KjtR4WSijmei1PZR/9po/5NxPJ0GNY1dIneEXnlfnu5/dPzBqFbXyvvggVUTcpiS2jAYSUOgm6DWSqMQZatX9ifHwXtfMrGXug7eLU/veyk8/FjjO9MkTSyDkaSONQtR/R72o/7yZO29yQxtC+Z3Vu7qW/PlsRV3wmYz4Vm7wtF7wfZbtt6uUSh694vg296VJk0og5GkKa9RMGvUOjahzj8RfvvNtsXueADOuTL3E/rSD0bYa/7iTcpEwL5PyX2TjlkGh+4Oczt4vNSczcZy4JJaMRhJ0lhs84zGwWjmPNjhcFh4LOx4LAu3XsYfRPAHZXVHQW3do3D3+XDHOXDH2XD/ZQ2L7XP8X5Le/VHAvmLSRDEYSdJY7PZWuPdiuOcXMP+psOOxOQxtfzDMnDO+umdtDjs+N08Aq++CO34EP3/lxuW2O/B3s71qGasPXPWd+KXpxmAkSWMxZ1s4/NuTsxOjk/sAABm3SURBVK+5O8BTT4KnvAJI8PD18NBy2OmEnu96Ii9TGqQ0FRiMJGmqiAACtnpanqaYXo8dVt1Po5auRtvUl212jM1CXauWs1ZDYjQ7nnb7aDfMRqfjtVXXT9R4ctOFwUiSNOWNZXywblq+xjMeW6eho5t9jHefkz2221Ti4PKSJEmFwUjD69GV8MBVsKHJ4857bdUt8ODVsGF9f/YvSdqEl9I0nK7/DFzyNkgbYLOt4IlHlbuAjoMtdunNPh9/CO48D24/C27/ITxyQ37/iUfDkT+AGf44SlK/+ZtYw+nKv8mhCHJgWfk/eQLYYkkOSQufCwuPysFpLDash/t+lYPQHT/Mt3WnBq1Dd56bx6x54hFj248kacIYjDScHru1+bpHboDr/zlPMTOPS7OwjCmz7f4wY2bzbVf9NrcG3fHDPDjf2vs7O55HWxyPJGnSGIykVtL63Jpz9/lwxftg9gJYeMxoUJqzHdz549Ew9NC1/T5iSdI4GIwkyEHngcvyCMOtrL0/Pwaig2dkNTRjNjzhUHjgclhz79jqkCT1jMFIAnjm38PWe8IDv8mtP7eflVuJNqwdf91bLc2dund8LuzwHJg1H76/j8FIkgaQwUiqiRmwYB/Y8mn5dWR3WPGZ8dX5hMNgyevzM7Q2f9LEHKckqWcMRhLk8YRqt9Hf/VNYv3pi6r37Z3kC2HrZaMvR+scmpn5J0oQyGEkAF7yiu/Jb7AJzts/zG9bCA1dCWtd6mwevzNO1f99gpUPyS9IgMBhJndhsa1h49OjdaFvsvPH6xx8ugzeWu9Mevr67+n/xh7DzH0zc8UqSxsRgpOEUs1q38MQM2O5Z+dLXwufCdge0Hpl6sy1h0YvyBPDITXDH2WVwx3Ph8QdbH8/We3X/NUiSJpzBSMNp4dE5tFTNf+poH6AnHgWztxl7/VvsDLu+MU8b1sG9F+eWpNt/CPdeODrqds2Ox419X5KkCWMw0nA65GtwxQdgzX2w/bNyq9CWu0LExO9rxix4wsF5evppsPYB+O7usObu0TI7Hj/x+5Ukdc1gpOE0ewHs94k+7XsbeFkZSDKlHJDm7tCfY5EkbWRGvw9AGmoRhiJJGiAGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBUGI0mSpMJgJEmSVBiMJEmSCoORJElSYTCSJEkqDEaSJEmFwUiSJKkwGEmSJBWTFowiYvuIODUiLoiIOyJiTUTcFhG/jIiPRsTBHdRxcER8OSJujojVEXF7RPwgIk7q8lhOioizyvarS31fjoiDxv4VSpKkqW7WZOwkIl4O/DOwXd2qHct0ILAb8JIWdbwPOI2Nw9zCMh0XEa8CTkwprW5Rx1zgm8AJdaueUqZXRcT7U0of7OTrkiRJ00vPW4wi4jXA18mh6C7gA8CxwH7AC4A/Ac4GHm9RxxvKdjOAG4DXk8PUS4DzSrEXAp9vczhfYDQUnVe2P7DUd0Op/6/L/iRJ0pCJlFLvKo9YClwKzAF+BrwwpfRgk7KzU0prG7y/DXATsA3wW2C/lNI9lfUzgW+RgxHA4Smlnzao53Dgx2Xxu8DvpZTWV9ZvD/wKeDJwP7BLSumBrr7gNiJiETACMDIywqJFiyayekmSpq2VK1eyePHi2uLilNLKXuyn1y1GnyKHonuAlzYLRQCNQlHxR+RQBPDOaigq260H3grUQs47mtRzanldD7y1GopKPfcA7yyLC8itSJIkaYj0LBhFxB7A0WXxH+sDTRdq/Y4eAs5oVKCkxnPK4rERsUXdsWxROZazW6TMM8p+AF46xuOVJElTVC9bjF5emf9mbSYiFkTEbhFR3xF7ExExm9wHCOAXLVqVAH5SXucAB9StO7C8Xy23iVL/hbVtImKzdscoSZKmj14Go9qt7w8C10TEqyPicuA+YDlwT0TcGBGn1bfwVOzG6J1z17bZX3X90rp1S5uUa1XPrLL/jkXEolYT+Q46SZI0oHp5u/6e5fVmcl+jtzUoszPwfuD3I+K4lNJtdesXV+bbdbIaabLdeOu5uk35ZttKkqQpppctRtuW1z3IoegB4M3ADsBc8uWuM0uZZcA3I6L+eLaszD/SZn+rKvP1LVATVY8kSZrGetliNL+8ziHfCfa8lNKFlfWXRMQJwPeA5wHPJnd4/q9KmbmV+Vb9iwDWVObn1a2bqHraqW+pqrcQuLjLOiVJ0iTpZTBazWg4+mZdKAIgpbQhIt5BDkYAr2TjYFQdxXp2m/3Nqcw/1uBYJqKeltqNqRAR3VQnSZImWS8vpT1cmT+zWaGU0lXArWWx/m6yah3tLmvNr8zXXy6bqHokSdI01stgVO2I3GmH5x3q3q9u126Y6OplrPpO0BNVjyRJmsZ6GYyuqszPbFO2tn5d3fvLGR3Reo82dVTXX1O37uom5VrVsw5Y0aasJEmaRnoZjKrPK1vSpuwu5fXW6ptlwMWLyuLBZcDHZg4vr2uAS+rWXcxop+vDaaLUXxt/6eI2A0pKkqRpppfB6DvA42W+6eM1ysNda6Ng/6xBkf8pr1s1q6cMnnhMWTw3pVTtU0RZPrcsHlPKN/LSsh/ID6aVJElDpGfBKKV0L/D5snhsRJxUXyYitgQ+UXnrsw2q+jx59GyAv6t/lEhEzAT+idHLcR9rcki192cBny7bVevZHvhIWXygcuySJGlI9LLFCOA04Ldl/ssR8amIODIi9ouIk8mXyfYp6/85pbTJGD8ppfsYfer9U4BfRsRrI2L/iHgRcDbwwrL+ayml8xodSErpR8DXy+KLgLMj4kWlnteSn5H25LL+XSml+8f6RUuSpKkpUkq93UHEUvJltV1bFPtX4M0ppcebFYiIDwDvBZoNBvR94GUppdVN1hMR88jjJD2/SZENwAdTSu9vcaxjVi7hjQCMjIywaFG7G+QkSRLAypUrWbz4dzeOL243duBY9brFiJTSNeRWoXcAvyQ/RHYt+Rb6bwBHpZRe3yoUlXpOAw4FvkoOF2uBu8gtRq9KKb2gVSgqdTyWUnoB8Oqy3V2lnpFS76G9CkWSJGnw9bzFSKNsMZIkaWymTYuRJEnSVGEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkgqDkSRJUmEwkiRJKgxGkiRJhcFIkiSpMBhJkiQVBiNJkqTCYCRJklQYjCRJkopZ/T6AITOzNnP77bf38zgkSZpS6v5uzmxWbrwipdSrulUnIvYHLu73cUiSNMUdkFK6pBcVeyltcu3Q7wOQJEnNeSltcl1bmT8IuLVfB6KuLWS0te8A4I4+Hos653mbujx3U1evzt1M4All/ooJqnMTBqPJtbYyf2tKaWXfjkRdiYjq4h2eu6nB8zZ1ee6mrh6fu1smsK6GvJQmSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkSYXBSJIkqTAYSZIkFT4SRJIkqbDFSJIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkSYXBSJIkqTAYSZIkFQajSRIRT46Ij0XENRGxKiLui4iLIuIvI2Lzfh/fdBERz4yI90TEmRExEhFrIuKRiFgeEV+MiMO6rO/4iDgjIlaWulaW5eO7qGNWRLwpIn4aEXdHxGMRsSIiPhMRe3b/VQ6XiDg9IlJlOqKDbTxvfRAR20fEqRFxQUTcUT772yLilxHx0Yg4uIM6Do6IL0fEzRGxOiJuj4gfRMRJXR7LSRFxVtl+danvyxFx0Ni/wukpImZHxOvL53x75ffmdRHxr51+ZtPm5y6l5NTjCXgB8ACQmkzXArv0+zin+gT8pMVnXJ3+HZjdpq4APtumns9SnjfYop7tgAtb1LEaeF2/P7tBnYC9gcfrPrMjPG+DNwEvB+5p89n/T5s63gesb7H9d4C5beqYC3y3RR3rgff2+/MalAlYDPymg9+bH2/2czPdfu76flKm+1R+sa8qJ/Vh4D3AwcBRwOcqJ/waYIt+H+9UnoAV5bO8FfgE8DLgAOAg4O3Aysrn/dU2dX2oUvbXwEmlrpPKcm3d37SoYyYbh7X/Bo4HDgROAe4s768Djuv35zdoE7lF+6LyGd1Z+RyP8LwN1gS8htFAcyfwfuAY4JnA88vn9kPgmy3qeEPlM18BvK6cuxcDP6qs+482x/KVStkfle0PKPWtqKx7Q78/t35PwCw2DkWXA/+n/M48FvgA8Ehl/Tua1DOtfu76fmKm+wScV07m48DBDda/o/KN8L5+H+9UnoDvAScCM5us3x64rvJ5H9ak3K6MtlJcDMyrW795eb92Xpc0qefkyr4+3WQ/D5b1y4FZ/f4MB2kC/ozRfxo+XPksj/C8Dc4ELCX/J5+AnwJbtyjbsKUW2Aa4v9RxC7B93fqZ5Nai2nl5TpN6Dq+U+U7974LyO+CWsv4+YJt+f359Pncvq3xeP2/0uxPYD1hb+cxm1a2fdj93fT8x03kiJ+baif5MkzIzgKsr33Sb9fu4p/MEnFA5J59sUubTlTIHNSlzUKXMp5qUuapyXjdvUuZdlXpe1u/PZ1AmcvP+w+VzOYLcAtEuGHne+nOuzimfw931gaaLOqr/IJ7UpMwicmtBAr7bpMz/MtqqsKhJmZMq+/qLfn9+fT53H698Fi9sUe6MSrlldeum3c+dna976yWV+X9rVCCltIHc5wVgAfmPgHrnx5X5JfUrIyLITe8A16aULmxUSXn/urL4krJdtZ7dgFpHwW+klB5tcjxfrMy/tOWRD5d/ArYAvpRS+nG7wp63/oiIPYCjy+I/ppTuGWNVtd+VD5H/CG8ipbSSHMIAjo2ILeqOZYvKsZxdyjdyRtkPDPG5K2ZX5m9sUe6Gyvyc2sx0/bkzGPVW7Q6oVcCvWpT7SWX+0N4djtj4F8GGBut3BnYq8z9psL6qtn4R8NS6dYc1KLeJlNId5GZh8NwDEBEnklv27iO3JHTC89YfL6/Mf7M2ExELImK3iNiuXQURMZvcjwTgFymltS2K187JHHKLfNWBjP7RbnXu1pI7+AIcGBGbtTvGaWx5ZX6XFuVq/0Qm4PrK+9Py585g1FtLy+uKlNK6FuWubbCNeuPwyvy1DdYvbbOeJuvrz9tY6lkcEfPblJ3WImIb4JNl8Z0ppbs73NTz1h+127gfBK6JiFdHxOXkULscuCciboyI0+pbeCp2I3cChsk/d7PK/ofV1xhtPXtnRMysLxAR+5LvrAb4ekrpocrqaflzZzDqkYiYS+7oB/luqKZSSveTW5Ug961QD0TEDPI16pr/bFCs+vm3PG/ASJPtxlpPkP+bGmanAwvJHUG/0MV2nrf+qF3+uBn4FPAfwDPqyuxM7iP2i4h4UoM6+nnuGtUzNMo/HicDjwGHABdHxGsi4qCIOCYiTiO34MwGLgP+vK6KaflzZzDqnS0r8490UL4WjJr9V6XxezujTfbfSild0qBMN+dtVWW+/rxNVD1DIyIOJd+yvQ54cyq9LTvkeeuPbcvrHsDbyOO1vRnYgTye0AHAmaXMMuCb5R+UKs9dH6WUvgXsT/5HZB/gS8AvgLPJgfZRciA6tFzKqpqW585g1DtzK/OtrpnXrCmv83pwLEMvIg4H/q4s3gW8pUnRbs7bmsp8/XmbqHqGQuln8jnyf4H/kFK6ossqPG/9UbuUMYc8jtHzUkqfTSndnVJaU/75OIHRcPRsNu0067nro9LH6lXAC8k/f/WeCLySxjcGTctzZzDqndWV+dlNS42qdRp8rAfHMtQiYi/gW+T+BGuAE1NKdzYp3s15m1OZrz9vE1XPsHgPuZ/Bb8mDynXL89Yf1c/rm43uSip33lY70b+yRR2eu0lU+uicA/wVedTp08k/h3OArYHnAueTW/6+GxF/WlfFtDx3BqPeebgy30lzX+0/r04uu6lDEbEzecTdBeT/aF+ZUmp190Q3563a8a/+vE1UPdNeueX73WXxlJTSqlblm/C89Uf18zqzWaGU0lXkEelh07vJPHf98wHgOWX+9Smld6aUrk0prU0pPZRSOhs4kjxQcQAfj4hqH7Jpee5mtS+isUgprY6Ie8gdsFt2EIuIBYye7JFWZdW50tHzHOBJ5NtMX1eup7dS7fjXrmNftcNg/Xmrr6fV+C61ehLtOx5OR28n/5d4I7B5k4eFLqvMHxURC8v8d0uQ8rz1xwi5szx01ml2J3L/o6penbtGfQg7qWcolLGEXlsWl6eUvtSoXEppXUS8l9xyNKNs8/ayelr+3BmMeusa8vgMu0bErBa37O9Rt43GKSK2J3cerI3NcUpK6d9bbFJzdWV+j6alNl1ff97q67msg3pGxthaMtXVmsZ3Id8+3M57K/M7kztjet764ypGW4A2udW7Tm19/e/B5eTW3JlM7LnrpJ515OenDaMnMtp5/tI2Zavj8FU/22n5c+eltN46v7zOJz9vppnq2DoX9O5whkNEbA2cxeitxO9KKX26w81vAm4r84e3KshoE/St5NuVq86vzDetp7R87F4WPfdj53nrj59W5jcZSb5O7Z+UW6tvlgEXLyqLB5eO+M3UzskaNm0RupjRjrutzt1sRsdfurjNgJLTWTWgtmskqQ6CWd1uWv7cGYx6638q869tVKDcuvqasvgA+VquxigiNic/L+mZ5a0PpZQ+0un25Rbxb5fFPSLioEblyvu1/1y+XX9reUppOaP/FZ1YjquRkyvz7S7zTUsppZNTStFqYuMO2UdW1t1c6vC89cd3yA8GhRaPaCh3hdZGwf5ZgyK135VbNasnIhYBx5TFc1NK1X4plOVzy+IxpXwjLy37geE+d/cxOrjjwRHRKhxVw8pNtZlp+3PXq4ewOf3uoXc/ZfSpwgc3WF99eOL7+328U3ki91M5q/J5fmKM9exO66dFz2Pjp0Xv1qSe11WO5R8brF/C6NOiVzDkT2lvc07eX/ksj/C8Dc5Efq5d0wfAkseoubRS5oAGZbYl/2OYyK0J29Wtn0kOYbU6jmxyLEdVynybuqfFk/t83lLW3w8s6Pfn1+dz99XK53VakzILGH3AawKeW7d+2v3c9f3ETPcJ2Jc8QFYi97x/N7kZ90jgs5VvhOuALft9vFN5Av678nmeCzyd3Gm32bR7i7r+tlLXr4FXkAdBe0VZrq37cIs6ZpKbiGtl/ws4jjzI5B8Dd5b3a+O/9P0zHNSJDoKR561v5+YJlbDxOHkE7CPJ3QdOJrcE1D7Lf2pRz5sq5VaQW9n3B14E/Kiy7qttjudrlbI/KtvvX+pbUVn3pn5/dv2eyK04qyqfyXeAl5W/WweTO1nfUll/TpN6ptXPXd9PzDBM5IGzHqyc8PrpOmDXfh/nVJ9afL7Npptb1DWDPBJsq+0/D8xoc0zbk/tPNKtjDfBH/f7sBn2i82DkeevP+VlKfrhoq8/9C8Bmber5APnhzs3q+F9gbps65pVyzepYj63z1c/rGODuDn5fnkuTFrbp9nPX95MyLBPwFODj5BC0ityMezFwKrB5v49vOkwd/GB3HIwqdT6f3P/h1vJDeWtZ7vg/FnLHxreQ+1bcQx6U7AbySM979ftzmwoTHQYjz1tfz9F84C/JT66/t3zuI8DXaXLpq0k9zwa+Qh7scw25peCH5DHIujmeV5Xt7iz1/LbUu0mXhmGfyP2/TiX3cb2L3In9UfIQGt8AXgxEB/VMi5+7KAciSZI09LwrTZIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkSYXBSJIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkSYXBSJIkqTAYSZIkFQYjSZKkwmAkSZJUGIwkSZIKg5EkSVJhMJIkSSoMRpIkScX/B5b2Ps7pJmikAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction\n",
    "fig, ax = plt.subplots()\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(bounding_boxes):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
