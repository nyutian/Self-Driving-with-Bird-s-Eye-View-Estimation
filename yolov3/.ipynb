{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [3, 3]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "#!conda install -c conda-forge opencv -y\n",
    "import cv2 \n",
    "from utils import *\n",
    "from utils.utils import *\n",
    "from models import Darknet\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "!pip install shapely\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "random.shuffle(labeled_scene_index)\n",
    "labeled_scene_index_train = labeled_scene_index[0:21]\n",
    "labeled_scene_index_val = labeled_scene_index[21:28]\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "print(\"Train scenes: {} \\nVal scenes: {}\".format(len(labeled_scene_index_train), len(labeled_scene_index_val)))\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_train,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index_val,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = \"cfg/yolov3-tiny-3cls.cfg\"\n",
    "config = \"cfg/yolov3-spp.cfg\"\n",
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 2.4,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'lr0': 0.0001,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 0.00005,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.000484,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(config, verbose=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "for k, v in dict(model.named_parameters()).items():\n",
    "    if '.bias' in k:\n",
    "        if v.is_leaf:\n",
    "            pg2 += [v]  # biases\n",
    "    elif 'Conv2d.weight' in k:\n",
    "        pg1 += [v]  # apply weight_decay\n",
    "    else:\n",
    "        pg0 += [v]  # all else\n",
    "\n",
    "optimizer = optim.Adam(pg0, lr=hyp['lr0'])\n",
    "optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
    "optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
    "del pg0, pg1, pg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "def train(train_loader, model, optimizer, criterion, epoch, sixinput):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(train_loader):\n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "\n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        batch_size = sample.shape[0]\n",
    "        sample = sample.mean(1).view(batch_size, 3, 256, 306)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "\n",
    "        # Make input the correct shape\n",
    "        if sixinput==False:\n",
    "            sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "\n",
    "        # Run through model\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample)\n",
    "\n",
    "        # Calculate loss and take step\n",
    "        loss, loss_items = compute_loss(output, target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_loader, model, sixinput):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for batch_idx, (sample, target, road_image) in enumerate(val_loader):\n",
    "        \n",
    "        # Rework target into expected format:\n",
    "        # A tensor of size [B, 6]\n",
    "        # where B is the total # of bounding boxes for all observvations in the batch \n",
    "        # and 6 is [id, class, x, y, w, h] (class is always 0, since we're not doing classification)\n",
    "        # Target is originally front left, front right, back left and back right\n",
    "        # Note: for boxes not aligned with the x-y axis, this will draw a box with the same center but a maximal width-height that *is* aligned\n",
    "        # The original range is xy values from from -40 to 40. We also rescale so that x values are from 0 to 1\n",
    "        # \"Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\"\n",
    "        target_yolo = torch.zeros(0,6)\n",
    "        for i, obs in enumerate(target):\n",
    "            boxes = (obs['bounding_box'] + 40)/80\n",
    "            boxes_yolo = torch.zeros(boxes.shape[0], 6)\n",
    "            for box in range(boxes.shape[0]):\n",
    "                cls = 0\n",
    "                x_center = 0.5*(boxes[box, 0, 0] + boxes[box, 0, 3])\n",
    "                y_center = 0.5*(boxes[box, 1, 0] + boxes[box, 1, 3])\n",
    "                width = max(boxes[box, 0, :]) - min(boxes[box, 0, :])\n",
    "                height = max(boxes[box, 1, :]) - min(boxes[box, 1, :])\n",
    "                boxes_yolo[box] = torch.tensor([i, cls, x_center, y_center, width, height])\n",
    "            target_yolo = torch.cat((target_yolo, boxes_yolo), 0)\n",
    "        \n",
    "        # Send to device\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        batch_size = sample.shape[0]\n",
    "        samples = torch.unbind(sample, 1)\n",
    "        encodeds = []\n",
    "        for sample in samples:\n",
    "            encoded = model_enc.encoder(sample)\n",
    "            encodeds.append(encoded)\n",
    "        encode = torch.mean(encodeds, dim=1)\n",
    "        encode = encode.view(batch_size, 3)\n",
    "        #sample = sample.mean(1).view(batch_size, 3, 256, 306)\n",
    "        target_yolo = target_yolo.to(device)\n",
    "         \n",
    "        # Make input the correct shape\n",
    "#         if sixinput==False:\n",
    "#             batch_size = sample.shape[0]\n",
    "#             sample = sample.view(batch_size, -1, 256, 306) # torch.Size([3, 18, 256, 306])\n",
    "        \n",
    "        # Run through model\n",
    "        with torch.no_grad():\n",
    "            output = model(enc)\n",
    "        # Calculate loss\n",
    "        #print(output[0].shape)\n",
    "        loss, loss_items = compute_loss(output[1], target_yolo, model, hyp) # Note: this is defined in yolov3.py\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    loss = sum(losses)/len(losses)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = np.inf\n",
    "#val_threat_score_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, optimizer, None, epoch, sixinput=False)\n",
    "    \n",
    "    # Evaluate at the end of the epoch\n",
    "    print(\"Evaluating after Epoch {}:\".format(epoch))\n",
    "    #val_loss, val_threat_score = evaluate(valloader, model, loss, sixinput=False)\n",
    "    #print(\"Val loss is {:.6f}, threat score is {:.6f}\".format(val_loss, val_threat_score))\n",
    "    model.training= False\n",
    "    val_loss = evaluate(valloader, model, sixinput=False)\n",
    "    model.training=True\n",
    "    print(\"Val loss is {}\".format(val_loss.cpu().detach()))\n",
    "    \n",
    "    # If this is the best model so far, save it\n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'config': config,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            }, '../models/best_bounding_box.pt')\n",
    "    \n",
    "    # Save loss \n",
    "    val_loss_hist.append(val_loss)\n",
    "    #val_threat_score_hist.append(val_threat_score)\n",
    "\n",
    "#checkpoint = torch.load('models/best_bounding_box.pt')\n",
    "#checkpoint['val_loss_hist'] = val_loss_hist\n",
    "#checkpoint['val_threat_score_hist'] = val_threat_score_hist\n",
    "#torch.save(checkpoint, 'models/best_bounding_box.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = 3*256*306\n",
    "d = 20\n",
    "s = 300\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size= (7,3), stride= 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size= 3, stride= 3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size= 3, stride= 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size= 5, stride= 3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size= 3, stride= 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2,True)\n",
    "        )\n",
    "        self.l1=nn.Sequential(\n",
    "            nn.Linear(1024*48, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, s * 2)\n",
    "        )\n",
    "        self.l2=nn.Sequential(\n",
    "            nn.Linear(s, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, 1024*48)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=(7,3), stride=3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        mu_logvar = self.l1(enc.view(len(x),-1)).view(-1, 2, s)\n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        dec = self.decoder(self.l2(z).view(len(z),1024, 6, 8))\n",
    "        return dec, mu, logvar\n",
    "\n",
    "model_enc = VAE().to(device)\n",
    "checkpoint = torch.load('/home/yt1526/VAE')\n",
    "model_enc.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
